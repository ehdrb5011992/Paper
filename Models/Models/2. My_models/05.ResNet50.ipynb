{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vwVmRuFcUBkT"
   },
   "source": [
    "# [ResNet50]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0MMz6DhUBkW"
   },
   "source": [
    "*KU LeeDongGyu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ywk25Fr79dWe"
   },
   "source": [
    "## Contents\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KSON5xiR9dWf"
   },
   "source": [
    "1. Data Preprocessing\n",
    "```\n",
    "1) Data Import\n",
    "2) Data Augmentation\n",
    "```\n",
    "2. Support Functions & Almost Original ResNet\n",
    "```\n",
    "1) Support Functions\n",
    "2) Almost orginal ResNet\n",
    "```\n",
    "3. ResNet50\n",
    "```\n",
    "1) ResNet50\n",
    "2) ResNet50 Evaluate\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U01q4o40UBkY"
   },
   "source": [
    "### Install Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XjdygsS_UBke"
   },
   "source": [
    "### Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 56819,
     "status": "ok",
     "timestamp": 1599313666098,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "07323071725004325774"
     },
     "user_tz": -540
    },
    "id": "o1tpIlBhXG2i",
    "outputId": "024ad733-e0c8-4b9c-cc4a-4f6b8af31935"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 56791,
     "status": "ok",
     "timestamp": 1599313666099,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "07323071725004325774"
     },
     "user_tz": -540
    },
    "id": "IJdbC2nRXR6h",
    "outputId": "ceeca4a6-7955-4a89-d7eb-38abf65c3205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/Paper\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/My Drive/Colab Notebooks/Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I98omoQ8FF90"
   },
   "outputs": [],
   "source": [
    "from f1score import macro_f1score,weighted_f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKLMWqbuUBkf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as ks\n",
    "from tensorflow.keras import backend as K \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input, Concatenate, ZeroPadding2D ,GlobalMaxPooling2D, Reshape , Lambda , Add, Multiply\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, BatchNormalization, AveragePooling2D , ZeroPadding2D, SeparableConv2D\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop , SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping , LearningRateScheduler, ModelCheckpoint, CSVLogger, Callback, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l1,l2,l1_l2\n",
    "from tensorflow.keras.models import Model , load_model , Sequential\n",
    "from tensorflow.keras.utils import plot_model , to_categorical, get_file\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 59697,
     "status": "ok",
     "timestamp": 1599313669045,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "07323071725004325774"
     },
     "user_tz": -540
    },
    "id": "cbiFovMgXTax",
    "outputId": "d14ad9b3-42e9-4ab4-addc-881a8cae700c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/My Drive/Colab Notebooks/Paper'"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 59682,
     "status": "ok",
     "timestamp": 1599313669046,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "07323071725004325774"
     },
     "user_tz": -540
    },
    "id": "AcT0A_iwEzaZ",
    "outputId": "3359c35f-d8fc-42bd-8038-c12c70e64e38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(ks.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 65382,
     "status": "ok",
     "timestamp": 1599313674764,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "07323071725004325774"
     },
     "user_tz": -540
    },
    "id": "Gr5hMHvmABmG",
    "outputId": "390aa155-d659-42eb-d28c-369d09091245"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 65366,
     "status": "ok",
     "timestamp": 1599313674765,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "07323071725004325774"
     },
     "user_tz": -540
    },
    "id": "5QEPaq7XQBs8",
    "outputId": "f35f8053-9d52-4bf8-c677-4e9a2c77d9b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6630256943948353527\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 13094384070374356402\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 9580617487671179092\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15473775744\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 15534201890124306306\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OOsm86eVUBko"
   },
   "source": [
    "## 1. Data Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DgwOtB_QEhll"
   },
   "source": [
    "### 1) Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i5hoO5oVDDJh"
   },
   "outputs": [],
   "source": [
    "# 바꿔서 살펴 볼 것들\n",
    "# CALTECH, CIFAR100, FER, MIT\n",
    "data_name = 'CALTECH'\n",
    "gan_type = 'No_GAN'\n",
    "number = '1'\n",
    "size = 224 # sizes after cropping\n",
    "super_size = 256 # sizes before cropping \n",
    "input_sizes = (size,size,3)\n",
    "batch_sizes = 128\n",
    "weight_decay = 3e-3\n",
    "epochs = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SPnWxfGzLOF6"
   },
   "outputs": [],
   "source": [
    "# 참고 : https://stackoverflow.com/questions/32419510/how-to-get-reproducible-results-in-keras/52897216#52897216\n",
    "# setting the seed number for random number generation for reproducibility.\n",
    "\n",
    "from numpy.random import seed\n",
    "import random\n",
    "\n",
    "\n",
    "if number=='1':\n",
    "    seed_num = 200225\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_num)\n",
    "    random.seed(seed_num)\n",
    "    seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "elif number=='2':\n",
    "    seed_num = 727\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_num)\n",
    "    random.seed(seed_num)\n",
    "    seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "elif number=='3':\n",
    "    seed_num = 115\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_num)\n",
    "    random.seed(seed_num)\n",
    "    seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "elif number=='4':\n",
    "    seed_num = 501\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_num)\n",
    "    random.seed(seed_num)\n",
    "    seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "elif number=='5':\n",
    "    seed_num = 517\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_num)\n",
    "    random.seed(seed_num)\n",
    "    seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8588MeRdp1av"
   },
   "outputs": [],
   "source": [
    "# data import\n",
    "\n",
    "if data_name=='FER' :\n",
    "    x_train =  np.zeros(28698)\n",
    "    x_valid = np.zeros(3589)\n",
    "    x_test = np.zeros(3588)\n",
    "    classes = 7 \n",
    "    tr_center = [0.50793296, 0.50793296, 0.50793296]\n",
    "elif data_name=='MIT':\n",
    "    x_train = np.zeros(12466)\n",
    "    x_valid = np.zeros(1564)\n",
    "    x_test = np.zeros(1590)\n",
    "    classes = 67 \n",
    "    tr_center = [0.47916578, 0.42029615, 0.36046057]\n",
    "elif data_name=='CALTECH':\n",
    "    x_train = np.zeros(24510)\n",
    "    x_valid = np.zeros(2980)\n",
    "    x_test = np.zeros(3118)\n",
    "    classes = 257\n",
    "    tr_center = [0.51397761, 0.49525248, 0.46555727]\n",
    "elif data_name=='CIFAR100':\n",
    "    x_train = np.zeros(39941)\n",
    "    x_valid = np.zeros(10059)\n",
    "    x_test = np.zeros(10000)\n",
    "    classes = 100\n",
    "    tr_center = [0.53393271, 0.51324147, 0.46450563]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQlStjHWt-jM"
   },
   "outputs": [],
   "source": [
    "dir = os.path.join(os.getcwd(),data_name,gan_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XrNLBwoJCRR9"
   },
   "source": [
    "### 2) Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-eN010dp1a5"
   },
   "outputs": [],
   "source": [
    "# 참고 : https://jkjung-avt.github.io/keras-image-cropping/\n",
    "\n",
    "def random_crop(img, random_crop_size):\n",
    "    # Note: image_data_format is 'channel_last'\n",
    "    assert img.shape[2] == 3 # img.shape[2] 가 3(rgb)이 아니면 assertion error 발생\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    dy, dx = random_crop_size\n",
    "    x = np.random.randint(0, width - dx + 1)\n",
    "    y = np.random.randint(0, height - dy + 1)\n",
    "    return img[y:(y+dy), x:(x+dx), :]\n",
    "\n",
    "\n",
    "def crop_generator(batches, crop_length):\n",
    "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
    "    crops from the image batches generated by the original iterator.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
    "        yield (batch_crops, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EW0KZ6x9EBtf"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array,load_img\n",
    "import glob\n",
    "\n",
    "# 데이터 전체에 대해 centering 진행함.\n",
    "\n",
    "def read_cal_image(img_path): \n",
    "    x = img_to_array(load_img(img_path)) # x는 채널별 평균값\n",
    "    y = x.shape[0] * x.shape[1]# y는 데이터별 픽셀 수 (비중)\n",
    "\n",
    "    x = 1/255. * x # scaling하고, centering값을 뽑아냄.\n",
    "    x = np.mean(x, axis=(0,1))\n",
    "    \n",
    "    return np.hstack([x,y])\n",
    "\n",
    "def calculate_centered_mean(dataset_path,x_train=x_train):\n",
    "    num = len(x_train)\n",
    "    space = np.empty((num,4))\n",
    "    i=0\n",
    "\n",
    "    for p in glob.glob(os.path.join(dataset_path,'*/*.*')) :\n",
    "        space[i] = read_cal_image(p)\n",
    "        i += 1\n",
    "\n",
    "    ratio = space[:,3] / np.sum(space[:,3])\n",
    "\n",
    "    return np.average(space[:,0:3],axis=0,weights=ratio)\n",
    "\n",
    "\n",
    "# 아래의 함수를 돌려서 나온 결과값을 중심화 값으로 설정.\n",
    "\n",
    "# train_mean = calculate_centered_mean(os.path.join(dir,'data/train')).reshape((1,1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rn363NOip1a9"
   },
   "outputs": [],
   "source": [
    "datagen_tr = ImageDataGenerator(\n",
    "    rescale=1/255.,\n",
    "    horizontal_flip=True,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=False,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=[0.9,1.0],\n",
    "    fill_mode = 'nearest')\n",
    "datagen_val = ImageDataGenerator(rescale=1/255.,featurewise_center=True)\n",
    "datagen_tes = ImageDataGenerator(rescale=1/255.,featurewise_center=True)\n",
    "\n",
    "# 원래는 이 자리에 fit 매서드를 써야하지만, 그냥 내가 중심화함수를 만들고 적용함. \n",
    "\n",
    "# 중심화 설정\n",
    "datagen_tr.mean = np.array(tr_center, dtype=np.float32).reshape((1,1,3)) # RGB\n",
    "datagen_val.mean = np.array(tr_center, dtype=np.float32).reshape((1,1,3)) # RGB\n",
    "datagen_tes.mean = np.array(tr_center, dtype=np.float32).reshape((1,1,3)) # RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 126192,
     "status": "ok",
     "timestamp": 1599313735667,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "07323071725004325774"
     },
     "user_tz": -540
    },
    "id": "nT4DDIUNp1a_",
    "outputId": "d99edef5-81b1-451d-98ed-f7d0b2d61be7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24509 images belonging to 257 classes.\n",
      "Found 2980 images belonging to 257 classes.\n",
      "Found 3118 images belonging to 257 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = datagen_tr.flow_from_directory(directory=os.path.join(dir,'data/train'),target_size=(super_size,super_size),batch_size=batch_sizes,class_mode='categorical') # fer : 28698 / mit : 12466 / caltech : 24509 / cifar : 39941\n",
    "train_generator= crop_generator(train_batches, size)\n",
    "valid_generator = datagen_val.flow_from_directory(directory=os.path.join(dir,'data/valid'),target_size=(size,size),batch_size=batch_sizes,class_mode='categorical') # fer : 3589 / mit : 1564 / caltech : 2980 / cifar : 10059\n",
    "test_generator = datagen_tes.flow_from_directory(directory=os.path.join(dir,'data/test'),target_size=(size,size),batch_size=batch_sizes,class_mode='categorical') # fer : 3588 / mit : 1590 / caltech : 3118 / cifar : 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "afa9Vvwdw1te"
   },
   "source": [
    "## 2. Support Functions & Almost Original ResNet\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iER-OGdBw1tf"
   },
   "source": [
    "### 1) Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XA3sqFv_ZyBS"
   },
   "outputs": [],
   "source": [
    "# def lr_schedule(epoch):\n",
    "#     init_lr = 1e-4\n",
    "#     k = 0.04\n",
    "#     lr = init_lr * np.exp(-k*epoch)\n",
    "#     print('Learning rate: ', lr)\n",
    "#     return lr\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch < 60:\n",
    "        lr = lr\n",
    "    else :\n",
    "        lr = lr * 0.1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TjxgKIgDw1tj"
   },
   "outputs": [],
   "source": [
    "# 편의상 아래와 같은 resnet 묶음의 이름을 resnet_layer이라 하자 \n",
    "def resnet_layer(inputs, weight_decay=weight_decay, num_filters=16, kernel_size=(3,3), strides=(1,1),activation=None):\n",
    "\n",
    "    conv = Conv2D(num_filters,kernel_size=kernel_size,strides=strides, padding='same', kernel_initializer='he_uniform', kernel_regularizer=l2(weight_decay))\n",
    "    x = inputs\n",
    "    x = conv(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if activation is not None:\n",
    "        x = Activation(activation)(x) # conv-bn-activation\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u8Ss5eT5w1tm"
   },
   "outputs": [],
   "source": [
    "# resnet 코드를 만들기 앞서, 코드가 너무 길어 비효율적이므로 적당한 함수를 만든다.\n",
    "# block_v1 은 resnet18과 resnet34에 특화된 block이고, block_v2는 resnet50, resnet101, resnet152에 특화된 block이다.\n",
    "# 이 두개의 block은, 나중에 하나의 모형에서 같이 고려될 것이다.\n",
    "\n",
    "def block_v1(input_shape, num_filters, identi, half=True):\n",
    "    # identi : resnet의 존재 이유인, 더하게 될 자기자신값이다.\n",
    "    if half:\n",
    "        layer1 = resnet_layer(inputs=input_shape, num_filters=num_filters, strides=(2, 2), activation='relu')\n",
    "        layer2 = resnet_layer(inputs=layer1, num_filters=num_filters, strides=(1, 1), activation=None)\n",
    "    else:\n",
    "        layer1 = resnet_layer(inputs=input_shape, num_filters=num_filters, strides=(1, 1), activation='relu')\n",
    "        layer2 = resnet_layer(inputs=layer1, num_filters=num_filters, strides=(1, 1), activation=None)\n",
    "    \n",
    "    res_2_1 = add([identi, layer2])  # block 1\n",
    "    output = Activation(\"relu\")(res_2_1)\n",
    "    return output\n",
    "\n",
    "\n",
    "def block_v2(input_shape, num_filters, identi, half=True):\n",
    "    # identi : resnet의 존재 이유인, 더하게 될 자기자신값이다.\n",
    "    if half:\n",
    "        layer1 = resnet_layer(inputs=input_shape, num_filters=num_filters, strides=(2, 2), kernel_size=(1, 1), activation='relu')\n",
    "        layer2 = resnet_layer(inputs=layer1, num_filters=num_filters, strides=(1, 1), kernel_size=(3, 3), activation='relu')\n",
    "        layer3 = resnet_layer(inputs=layer2, num_filters= 4*num_filters, strides=(1, 1), kernel_size=(1, 1), activation=None)\n",
    "    else:\n",
    "        layer1 = resnet_layer(inputs=input_shape, num_filters= num_filters, strides=(1, 1), kernel_size=(1, 1), activation='relu')\n",
    "        layer2 = resnet_layer(inputs=layer1, num_filters= num_filters, strides=(1, 1), kernel_size=(3, 3), activation='relu')\n",
    "        layer3 = resnet_layer(inputs=layer2, num_filters= 4*num_filters, strides=(1, 1), kernel_size=(1, 1), activation=None)\n",
    "\n",
    "    res_3_1 = add([identi, layer3])  # block 1\n",
    "    output = Activation(\"relu\")(res_3_1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ikmjp50w1uA"
   },
   "source": [
    "### 2) Almost Orginial ResNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGdsPCI1w1uB"
   },
   "outputs": [],
   "source": [
    "# 좀더 깔끔하게 작성됨. 앞으로의 resnet모형은 다음의 함수에서 해결한다.\n",
    "def ResNet(input_shape=(224, 224, 3), classes=1000, weight_decay=weight_decay, num_filters=64, num_blocks=[0,0,0,0] ,version=None, name=None):\n",
    "    # num_blocks 는 list로 받는 인자. conv3_x ~ conv5_x의 block의 수를 앞에서부터 차례로 넣어주면 된다.\n",
    "    # num_filters 는 초기의 filters를 의미한다.\n",
    "    \n",
    "    if len(num_blocks) != 4 :\n",
    "        raise NameError(\"Please input the number of blocks from conv2_x to conv5_x in the 'num_blocks' variable.\")\n",
    "\n",
    "    if version == \"v1\" : # resnet 18, 34에 해당. (블록이 같음)\n",
    "        block = block_v1\n",
    "    elif version == \"v2\": # resnet 50, 101, 152에 해당. (블록이 같음)\n",
    "        block = block_v2\n",
    "    else :\n",
    "        raise NameError(\"Please input the string 'v1' or 'v2' in the 'version' variable. \")\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # conv1\n",
    "    layer1 = resnet_layer(inputs=inputs, num_filters=num_filters, strides=(2, 2), kernel_size=(7, 7), activation='relu')\n",
    "    cum_block = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same', name='pool1/3x3_s2')(layer1)\n",
    "    \n",
    "    # cum_block 변수에 하나씩 residual block을 누적시켜보자.\n",
    "\n",
    "    for stack in range(4):\n",
    "        if stack == 0 :\n",
    "            # 우리가 누적시킬 변수, cum_block을 정의하고, 이 변수에 누적시켜가면서 모형을 출력한다.\n",
    "\n",
    "            # 이 아래 for문은 conv2_x\n",
    "            # 들어가기 전에, block_v2라면 input과 output의 filter size를 동일하게 해주는 작업을 한다. 덧셈이 가능해야 하므로, 반드시!\n",
    "            if version == 'v2':\n",
    "                x = Conv2D(4 * num_filters, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_initializer='he_uniform', kernel_regularizer=l2(weight_decay))(cum_block)\n",
    "                x = BatchNormalization()(x) # 모든 conv.뒤에는 bn을 적용.\n",
    "                cum_block = block(input_shape=cum_block, num_filters=num_filters, identi=x, half=False)\n",
    "\n",
    "            if version != 'v2':\n",
    "                  new_num_blocks = num_blocks[stack]\n",
    "            else :\n",
    "                  new_num_blocks = num_blocks[stack]-1\n",
    "\n",
    "            for res_block in range(new_num_blocks):\n",
    "                cum_block = block(input_shape=cum_block, num_filters=num_filters, identi=cum_block, half=False)\n",
    "        else:\n",
    "\n",
    "            # 이 아래는 conv3_x ~ conv5_x 까지.\n",
    "            num_filters *= 2\n",
    " \n",
    "            if version == 'v2':\n",
    "                x = Conv2D(4 * num_filters, kernel_size=(1, 1), strides=(2, 2), padding='valid', kernel_initializer='he_uniform', kernel_regularizer=l2(weight_decay))(cum_block)  # block_v2라면, input에 해당하는 x를 덧셈이 가능하게 맞춰준다.\n",
    "                x = BatchNormalization()(x) # 모든 conv.뒤에는 bn을 적용.\n",
    "            else:\n",
    "                x = Conv2D(num_filters, (1, 1), strides=(2, 2), padding='valid', kernel_initializer='he_uniform', kernel_regularizer=l2(weight_decay))(cum_block)  # identi_input 에 해당 // rxc가 각각 반토막남.\n",
    "                x = BatchNormalization()(x) # 모든 conv.뒤에는 bn을 적용.\n",
    "                #즉, 위 x는 결국 이어질 conv 층의 input임.\n",
    "\n",
    "            for res_block in range(num_blocks[stack]):\n",
    "                if res_block == 0:\n",
    "                    cum_block = block(input_shape=cum_block , num_filters = num_filters , identi=x, half=True) # block 갱신\n",
    "                else:\n",
    "                    cum_block = block(input_shape=cum_block , num_filters = num_filters , identi=cum_block , half=False) # block 갱신\n",
    "\n",
    "    # 마지막 1층\n",
    "    y = GlobalAveragePooling2D()(cum_block)\n",
    "    outputs = Dense(classes, activation='softmax')(y)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name = name)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tOAp8LsWRMR6"
   },
   "source": [
    "## 3. ResNet50\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lOCs8l3URMSy"
   },
   "source": [
    "### 1) ResNet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CY0dRfL5RMS0"
   },
   "outputs": [],
   "source": [
    "# ResNet50\n",
    "model = ResNet(input_shape=input_sizes, classes=classes, num_blocks=[3,4,6,3], version='v2', name='ResNet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 128036,
     "status": "ok",
     "timestamp": 1599313737676,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "07323071725004325774"
     },
     "user_tz": -540
    },
    "id": "nBDXNAhgRMS3",
    "outputId": "2d4ac6eb-73ce-49e8-ccbd-a3d7d7351a8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 112, 112, 64) 9472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 112, 112, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 112, 112, 64) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool1/3x3_s2 (MaxPooling2D)     (None, 56, 56, 64)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 56, 56, 64)   4160        pool1/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 56, 56, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 56, 56, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 56, 56, 64)   36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 56, 56, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 56, 56, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 56, 56, 256)  16640       pool1/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 56, 56, 256)  16640       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 56, 56, 256)  1024        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 56, 56, 256)  1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 56, 56, 256)  0           batch_normalization_1[0][0]      \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 56, 56, 256)  0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 56, 56, 64)   16448       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 56, 56, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 56, 56, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 56, 56, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 56, 56, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 56, 56, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 56, 56, 256)  16640       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 56, 56, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 56, 56, 256)  0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 56, 56, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 56, 56, 64)   16448       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 56, 56, 64)   256         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 56, 56, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 56, 56, 64)   36928       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 56, 56, 64)   256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 56, 56, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 56, 56, 256)  16640       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 56, 56, 256)  1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 56, 56, 256)  0           activation_6[0][0]               \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 56, 56, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 28, 28, 128)  32896       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 28, 28, 128)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 28, 28, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 28, 28, 128)  147584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 28, 28, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 28, 28, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 28, 28, 512)  131584      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 28, 28, 512)  66048       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 28, 28, 512)  2048        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 28, 28, 512)  2048        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 28, 28, 512)  0           batch_normalization_11[0][0]     \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 28, 28, 512)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 28, 28, 128)  65664       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 28, 28, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 28, 28, 128)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 28, 28, 128)  147584      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 28, 28, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 28, 28, 128)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 28, 28, 512)  66048       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 28, 28, 512)  2048        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 28, 28, 512)  0           activation_12[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 28, 28, 512)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 28, 28, 128)  65664       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 28, 28, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 28, 28, 128)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 28, 28, 128)  147584      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 28, 28, 128)  512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 28, 28, 128)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 28, 28, 512)  66048       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 28, 28, 512)  2048        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 28, 28, 512)  0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 28, 28, 512)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 28, 28, 128)  65664       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 28, 28, 128)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 28, 28, 128)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 28, 28, 128)  147584      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 28, 28, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 28, 28, 128)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 28, 28, 512)  66048       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 28, 28, 512)  2048        conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 28, 28, 512)  0           activation_18[0][0]              \n",
      "                                                                 batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 28, 28, 512)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 14, 14, 256)  131328      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 14, 14, 256)  1024        conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 14, 14, 256)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 14, 14, 256)  590080      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 14, 14, 256)  1024        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 14, 14, 256)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 14, 14, 1024) 525312      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 14, 14, 1024) 263168      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 14, 14, 1024) 4096        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 14, 14, 1024) 4096        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 14, 14, 1024) 0           batch_normalization_24[0][0]     \n",
      "                                                                 batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 14, 14, 1024) 0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 14, 14, 256)  262400      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 14, 14, 256)  1024        conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 14, 14, 256)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 14, 14, 256)  590080      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 14, 14, 256)  1024        conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 14, 14, 256)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 14, 14, 1024) 263168      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 14, 14, 1024) 4096        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 14, 14, 1024) 0           activation_24[0][0]              \n",
      "                                                                 batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 14, 14, 1024) 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 14, 14, 256)  262400      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 14, 14, 256)  1024        conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 14, 14, 256)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 14, 14, 256)  590080      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 14, 14, 256)  1024        conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 14, 14, 256)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 14, 14, 1024) 263168      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 14, 14, 1024) 4096        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 14, 14, 1024) 0           activation_27[0][0]              \n",
      "                                                                 batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 14, 14, 1024) 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 14, 14, 256)  262400      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 14, 14, 256)  1024        conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 14, 14, 256)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 14, 14, 256)  590080      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 14, 14, 256)  1024        conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 14, 14, 256)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 14, 14, 1024) 263168      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 14, 14, 1024) 4096        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 14, 14, 1024) 0           activation_30[0][0]              \n",
      "                                                                 batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 14, 14, 1024) 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 14, 14, 256)  262400      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 14, 14, 256)  1024        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 14, 14, 256)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 14, 14, 256)  590080      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 14, 14, 256)  1024        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 14, 14, 256)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 14, 14, 1024) 263168      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 14, 14, 1024) 4096        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 14, 14, 1024) 0           activation_33[0][0]              \n",
      "                                                                 batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 14, 14, 1024) 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 14, 14, 256)  262400      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 14, 14, 256)  1024        conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 14, 14, 256)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 14, 14, 256)  590080      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 14, 14, 256)  1024        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 14, 14, 256)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 14, 14, 1024) 263168      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 14, 14, 1024) 4096        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 14, 14, 1024) 0           activation_36[0][0]              \n",
      "                                                                 batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 14, 14, 1024) 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 7, 7, 512)    524800      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 7, 7, 512)    2048        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 7, 7, 512)    0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 7, 7, 512)    2359808     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 7, 7, 512)    2048        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 7, 7, 512)    0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 7, 7, 2048)   2099200     activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 7, 7, 2048)   1050624     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 7, 7, 2048)   8192        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 7, 7, 2048)   8192        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 7, 7, 2048)   0           batch_normalization_43[0][0]     \n",
      "                                                                 batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 7, 7, 2048)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 7, 7, 512)    1049088     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 7, 7, 512)    2048        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 7, 7, 512)    0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 7, 7, 512)    2359808     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 7, 7, 512)    2048        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 7, 7, 512)    0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 7, 7, 2048)   1050624     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 7, 7, 2048)   8192        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 7, 7, 2048)   0           activation_42[0][0]              \n",
      "                                                                 batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 7, 7, 2048)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 7, 7, 512)    1049088     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 7, 7, 512)    2048        conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 7, 7, 512)    0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 7, 7, 512)    2359808     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 7, 7, 512)    2048        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 7, 7, 512)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 7, 7, 2048)   1050624     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 7, 7, 2048)   8192        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 7, 7, 2048)   0           activation_45[0][0]              \n",
      "                                                                 batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 7, 7, 2048)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 257)          526593      global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 24,114,305\n",
      "Trainable params: 24,061,185\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WgvSugB5DmgF"
   },
   "outputs": [],
   "source": [
    "# 폴더 생성\n",
    "\n",
    "os.makedirs(os.path.join(dir,'model_output',number,model.name), exist_ok=True)\n",
    "os.makedirs(os.path.join(dir,'train_valid_output',number), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L2w4kvkSAd6R"
   },
   "outputs": [],
   "source": [
    "# 참고 : https://github.com/OverLordGoldDragon/keras-adamw\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/Colab Notebooks/Paper')\n",
    "import utils\n",
    "import optimizers_v2\n",
    "from utils import get_weight_decays, fill_dict_in_order\n",
    "from utils import reset_seeds, K_eval\n",
    "from optimizers_v2 import AdamW, NadamW, SGDW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 128576,
     "status": "ok",
     "timestamp": 1599313738246,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "07323071725004325774"
     },
     "user_tz": -540
    },
    "id": "BR7JaCZuRMS6",
    "outputId": "dd770d31-fdd7-4dea-8e14-ce3b270286c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cosine annealing learning rates\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model=model, use_cosine_annealing=True, total_iterations = len(x_train) // batch_sizes , eta_min = 1e-2)\n",
    "#optimizer = Adam()\n",
    "filepath =  os.path.join(dir,'model_output',number,model.name,'{epoch:03d}.h5')\n",
    "\n",
    "callbacks_list = [ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_weights_only=False, save_best_only=True, mode='min'),\n",
    "                  ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_weights_only=False, save_best_only=True, mode='max'),\n",
    "                  #ReduceLROnPlateau(monitor='val_loss',patience=3,factor=0.1,min_lr=1e-5),\n",
    "                  LearningRateScheduler(lr_schedule,verbose=1)\n",
    "                  ]\n",
    "                  \n",
    "model.compile(optimizer, loss = 'categorical_crossentropy', metrics=['accuracy',macro_f1score,weighted_f1score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 44518334,
     "status": "ok",
     "timestamp": 1599358128011,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "07323071725004325774"
     },
     "user_tz": -540
    },
    "id": "fzHXIfGrRMS-",
    "outputId": "7b42c0cc-a4bd-43f2-d7b2-0e4ca06ad1a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 1/70\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_2/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_3/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_1/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_4/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_5/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_6/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_7/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_8/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_9/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_10/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_12/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_13/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_11/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_14/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_15/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_16/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_17/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_18/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_19/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_20/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_21/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_22/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_23/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_25/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_26/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_24/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_27/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_28/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_29/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_30/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_31/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_32/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_33/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_34/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_35/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_36/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_37/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_38/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_39/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_40/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_41/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_42/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_44/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_45/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_43/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_46/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_47/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_48/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_49/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_50/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_51/kernel:0\n",
      "0.0(L1), 0.0002170723834745943(L2) weight decay set for conv2d_52/kernel:0\n",
      "191/191 [==============================] - ETA: 0s - loss: 5.2439 - accuracy: 0.0822 - macro_f1score: 0.0016 - weighted_f1score: 3.1237e-05  \n",
      "Epoch 00001: val_loss improved from inf to 5.06383, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/001.h5\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.05944, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/001.h5\n",
      "191/191 [==============================] - 13675s 72s/step - loss: 5.2439 - accuracy: 0.0822 - macro_f1score: 0.0016 - weighted_f1score: 3.1237e-05 - val_loss: 5.0638 - val_accuracy: 0.0594 - val_macro_f1score: 1.6918e-04 - val_weighted_f1score: 1.3217e-06\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 2/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 4.6704 - accuracy: 0.1207 - macro_f1score: 0.0058 - weighted_f1score: 1.2491e-04\n",
      "Epoch 00002: val_loss improved from 5.06383 to 4.40921, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/002.h5\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.05944 to 0.14232, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/002.h5\n",
      "191/191 [==============================] - 445s 2s/step - loss: 4.6704 - accuracy: 0.1207 - macro_f1score: 0.0058 - weighted_f1score: 1.2491e-04 - val_loss: 4.4092 - val_accuracy: 0.1423 - val_macro_f1score: 0.0065 - val_weighted_f1score: 1.5183e-04\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 3/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 4.3550 - accuracy: 0.1514 - macro_f1score: 0.0089 - weighted_f1score: 1.9014e-04\n",
      "Epoch 00003: val_loss improved from 4.40921 to 4.21953, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/003.h5\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.14232 to 0.17357, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/003.h5\n",
      "191/191 [==============================] - 443s 2s/step - loss: 4.3550 - accuracy: 0.1514 - macro_f1score: 0.0089 - weighted_f1score: 1.9014e-04 - val_loss: 4.2195 - val_accuracy: 0.1736 - val_macro_f1score: 0.0110 - val_weighted_f1score: 2.3298e-04\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 4/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 4.0984 - accuracy: 0.1836 - macro_f1score: 0.0131 - weighted_f1score: 2.5442e-04\n",
      "Epoch 00004: val_loss improved from 4.21953 to 3.91563, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/004.h5\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.17357 to 0.21026, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/004.h5\n",
      "191/191 [==============================] - 445s 2s/step - loss: 4.0984 - accuracy: 0.1836 - macro_f1score: 0.0131 - weighted_f1score: 2.5442e-04 - val_loss: 3.9156 - val_accuracy: 0.2103 - val_macro_f1score: 0.0152 - val_weighted_f1score: 2.6515e-04\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 5/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 3.8658 - accuracy: 0.2124 - macro_f1score: 0.0178 - weighted_f1score: 3.2175e-04\n",
      "Epoch 00005: val_loss improved from 3.91563 to 3.74481, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/005.h5\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.21026 to 0.24592, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/005.h5\n",
      "191/191 [==============================] - 446s 2s/step - loss: 3.8658 - accuracy: 0.2124 - macro_f1score: 0.0178 - weighted_f1score: 3.2175e-04 - val_loss: 3.7448 - val_accuracy: 0.2459 - val_macro_f1score: 0.0217 - val_weighted_f1score: 3.7024e-04\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 6/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 3.6781 - accuracy: 0.2358 - macro_f1score: 0.0224 - weighted_f1score: 3.7663e-04\n",
      "Epoch 00006: val_loss improved from 3.74481 to 3.56994, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/006.h5\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.24592 to 0.27072, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/006.h5\n",
      "191/191 [==============================] - 446s 2s/step - loss: 3.6781 - accuracy: 0.2358 - macro_f1score: 0.0224 - weighted_f1score: 3.7663e-04 - val_loss: 3.5699 - val_accuracy: 0.2707 - val_macro_f1score: 0.0245 - val_weighted_f1score: 4.1942e-04\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 7/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 3.4709 - accuracy: 0.2692 - macro_f1score: 0.0290 - weighted_f1score: 4.5932e-04\n",
      "Epoch 00007: val_loss improved from 3.56994 to 3.45108, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/007.h5\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.27072 to 0.30197, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/007.h5\n",
      "191/191 [==============================] - 449s 2s/step - loss: 3.4709 - accuracy: 0.2692 - macro_f1score: 0.0290 - weighted_f1score: 4.5932e-04 - val_loss: 3.4511 - val_accuracy: 0.3020 - val_macro_f1score: 0.0346 - val_weighted_f1score: 5.1209e-04\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 8/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 3.3171 - accuracy: 0.2951 - macro_f1score: 0.0348 - weighted_f1score: 5.2172e-04\n",
      "Epoch 00008: val_loss improved from 3.45108 to 3.24928, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/008.h5\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.30197 to 0.32914, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/008.h5\n",
      "191/191 [==============================] - 450s 2s/step - loss: 3.3171 - accuracy: 0.2951 - macro_f1score: 0.0348 - weighted_f1score: 5.2172e-04 - val_loss: 3.2493 - val_accuracy: 0.3291 - val_macro_f1score: 0.0416 - val_weighted_f1score: 6.0428e-04\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 9/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 3.1141 - accuracy: 0.3210 - macro_f1score: 0.0440 - weighted_f1score: 6.1902e-04\n",
      "Epoch 00009: val_loss improved from 3.24928 to 3.02394, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/009.h5\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.32914 to 0.35326, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/009.h5\n",
      "191/191 [==============================] - 449s 2s/step - loss: 3.1141 - accuracy: 0.3210 - macro_f1score: 0.0440 - weighted_f1score: 6.1902e-04 - val_loss: 3.0239 - val_accuracy: 0.3533 - val_macro_f1score: 0.0505 - val_weighted_f1score: 7.1341e-04\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 10/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 2.9391 - accuracy: 0.3510 - macro_f1score: 0.0525 - weighted_f1score: 7.1843e-04\n",
      "Epoch 00010: val_loss improved from 3.02394 to 2.91551, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/010.h5\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.35326 to 0.37364, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/010.h5\n",
      "191/191 [==============================] - 450s 2s/step - loss: 2.9391 - accuracy: 0.3510 - macro_f1score: 0.0525 - weighted_f1score: 7.1843e-04 - val_loss: 2.9155 - val_accuracy: 0.3736 - val_macro_f1score: 0.0582 - val_weighted_f1score: 7.8364e-04\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 11/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 2.7771 - accuracy: 0.3812 - macro_f1score: 0.0619 - weighted_f1score: 8.2860e-04\n",
      "Epoch 00011: val_loss improved from 2.91551 to 2.84377, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/011.h5\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.37364 to 0.38791, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/011.h5\n",
      "191/191 [==============================] - 449s 2s/step - loss: 2.7771 - accuracy: 0.3812 - macro_f1score: 0.0619 - weighted_f1score: 8.2860e-04 - val_loss: 2.8438 - val_accuracy: 0.3879 - val_macro_f1score: 0.0684 - val_weighted_f1score: 9.1073e-04\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 12/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 2.6459 - accuracy: 0.4045 - macro_f1score: 0.0719 - weighted_f1score: 9.2375e-04\n",
      "Epoch 00012: val_loss improved from 2.84377 to 2.68162, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/012.h5\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.38791 to 0.41882, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/012.h5\n",
      "191/191 [==============================] - 447s 2s/step - loss: 2.6459 - accuracy: 0.4045 - macro_f1score: 0.0719 - weighted_f1score: 9.2375e-04 - val_loss: 2.6816 - val_accuracy: 0.4188 - val_macro_f1score: 0.0800 - val_weighted_f1score: 0.0010\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 13/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 2.4886 - accuracy: 0.4353 - macro_f1score: 0.0814 - weighted_f1score: 0.0010\n",
      "Epoch 00013: val_loss improved from 2.68162 to 2.62712, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/013.h5\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.41882 to 0.42425, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/013.h5\n",
      "191/191 [==============================] - 450s 2s/step - loss: 2.4886 - accuracy: 0.4353 - macro_f1score: 0.0814 - weighted_f1score: 0.0010 - val_loss: 2.6271 - val_accuracy: 0.4243 - val_macro_f1score: 0.0890 - val_weighted_f1score: 0.0011\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 14/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 2.3634 - accuracy: 0.4550 - macro_f1score: 0.0926 - weighted_f1score: 0.0011\n",
      "Epoch 00014: val_loss improved from 2.62712 to 2.57223, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/014.h5\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.42425 to 0.43886, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/014.h5\n",
      "191/191 [==============================] - 448s 2s/step - loss: 2.3634 - accuracy: 0.4550 - macro_f1score: 0.0926 - weighted_f1score: 0.0011 - val_loss: 2.5722 - val_accuracy: 0.4389 - val_macro_f1score: 0.0953 - val_weighted_f1score: 0.0012\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 15/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 2.2526 - accuracy: 0.4767 - macro_f1score: 0.1031 - weighted_f1score: 0.0012\n",
      "Epoch 00015: val_loss improved from 2.57223 to 2.44723, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/015.h5\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.43886 to 0.46026, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/015.h5\n",
      "191/191 [==============================] - 449s 2s/step - loss: 2.2526 - accuracy: 0.4767 - macro_f1score: 0.1031 - weighted_f1score: 0.0012 - val_loss: 2.4472 - val_accuracy: 0.4603 - val_macro_f1score: 0.1094 - val_weighted_f1score: 0.0013\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 16/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 2.1157 - accuracy: 0.5019 - macro_f1score: 0.1142 - weighted_f1score: 0.0014\n",
      "Epoch 00016: val_loss improved from 2.44723 to 2.43277, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/016.h5\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.46026 to 0.46128, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/016.h5\n",
      "191/191 [==============================] - 448s 2s/step - loss: 2.1157 - accuracy: 0.5019 - macro_f1score: 0.1142 - weighted_f1score: 0.0014 - val_loss: 2.4328 - val_accuracy: 0.4613 - val_macro_f1score: 0.1155 - val_weighted_f1score: 0.0014\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 17/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 2.0226 - accuracy: 0.5184 - macro_f1score: 0.1220 - weighted_f1score: 0.0014\n",
      "Epoch 00017: val_loss improved from 2.43277 to 2.34258, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/017.h5\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.46128 to 0.47962, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/017.h5\n",
      "191/191 [==============================] - 443s 2s/step - loss: 2.0226 - accuracy: 0.5184 - macro_f1score: 0.1220 - weighted_f1score: 0.0014 - val_loss: 2.3426 - val_accuracy: 0.4796 - val_macro_f1score: 0.1183 - val_weighted_f1score: 0.0014\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 18/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.9354 - accuracy: 0.5380 - macro_f1score: 0.1307 - weighted_f1score: 0.0015\n",
      "Epoch 00018: val_loss improved from 2.34258 to 2.32365, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/018.h5\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.47962 to 0.49355, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/018.h5\n",
      "191/191 [==============================] - 438s 2s/step - loss: 1.9354 - accuracy: 0.5380 - macro_f1score: 0.1307 - weighted_f1score: 0.0015 - val_loss: 2.3237 - val_accuracy: 0.4935 - val_macro_f1score: 0.1270 - val_weighted_f1score: 0.0015\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 19/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.8273 - accuracy: 0.5576 - macro_f1score: 0.1404 - weighted_f1score: 0.0016\n",
      "Epoch 00019: val_loss improved from 2.32365 to 2.26842, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/019.h5\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.49355 to 0.49423, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/019.h5\n",
      "191/191 [==============================] - 440s 2s/step - loss: 1.8273 - accuracy: 0.5576 - macro_f1score: 0.1404 - weighted_f1score: 0.0016 - val_loss: 2.2684 - val_accuracy: 0.4942 - val_macro_f1score: 0.1275 - val_weighted_f1score: 0.0015\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 20/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.7382 - accuracy: 0.5767 - macro_f1score: 0.1489 - weighted_f1score: 0.0017\n",
      "Epoch 00020: val_loss improved from 2.26842 to 2.25793, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/020.h5\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.49423 to 0.50170, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/020.h5\n",
      "191/191 [==============================] - 432s 2s/step - loss: 1.7382 - accuracy: 0.5767 - macro_f1score: 0.1489 - weighted_f1score: 0.0017 - val_loss: 2.2579 - val_accuracy: 0.5017 - val_macro_f1score: 0.1376 - val_weighted_f1score: 0.0016\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 21/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.6492 - accuracy: 0.5951 - macro_f1score: 0.1585 - weighted_f1score: 0.0018\n",
      "Epoch 00021: val_loss improved from 2.25793 to 2.18932, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/021.h5\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.50170 to 0.51596, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/021.h5\n",
      "191/191 [==============================] - 415s 2s/step - loss: 1.6492 - accuracy: 0.5951 - macro_f1score: 0.1585 - weighted_f1score: 0.0018 - val_loss: 2.1893 - val_accuracy: 0.5160 - val_macro_f1score: 0.1388 - val_weighted_f1score: 0.0016\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 22/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.5588 - accuracy: 0.6085 - macro_f1score: 0.1655 - weighted_f1score: 0.0019\n",
      "Epoch 00022: val_loss improved from 2.18932 to 2.17270, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/022.h5\n",
      "\n",
      "Epoch 00022: val_accuracy improved from 0.51596 to 0.52038, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/022.h5\n",
      "191/191 [==============================] - 425s 2s/step - loss: 1.5588 - accuracy: 0.6085 - macro_f1score: 0.1655 - weighted_f1score: 0.0019 - val_loss: 2.1727 - val_accuracy: 0.5204 - val_macro_f1score: 0.1453 - val_weighted_f1score: 0.0017\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 23/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.5005 - accuracy: 0.6246 - macro_f1score: 0.1715 - weighted_f1score: 0.0019\n",
      "Epoch 00023: val_loss did not improve from 2.17270\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.52038\n",
      "191/191 [==============================] - 439s 2s/step - loss: 1.5005 - accuracy: 0.6246 - macro_f1score: 0.1715 - weighted_f1score: 0.0019 - val_loss: 2.1935 - val_accuracy: 0.5204 - val_macro_f1score: 0.1435 - val_weighted_f1score: 0.0017\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 24/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.4003 - accuracy: 0.6441 - macro_f1score: 0.1825 - weighted_f1score: 0.0020\n",
      "Epoch 00024: val_loss did not improve from 2.17270\n",
      "\n",
      "Epoch 00024: val_accuracy improved from 0.52038 to 0.52310, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/024.h5\n",
      "191/191 [==============================] - 446s 2s/step - loss: 1.4003 - accuracy: 0.6441 - macro_f1score: 0.1825 - weighted_f1score: 0.0020 - val_loss: 2.1852 - val_accuracy: 0.5231 - val_macro_f1score: 0.1478 - val_weighted_f1score: 0.0017\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 25/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.3371 - accuracy: 0.6581 - macro_f1score: 0.1889 - weighted_f1score: 0.0021\n",
      "Epoch 00025: val_loss improved from 2.17270 to 2.09717, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/025.h5\n",
      "\n",
      "Epoch 00025: val_accuracy improved from 0.52310 to 0.54212, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/025.h5\n",
      "191/191 [==============================] - 448s 2s/step - loss: 1.3371 - accuracy: 0.6581 - macro_f1score: 0.1889 - weighted_f1score: 0.0021 - val_loss: 2.0972 - val_accuracy: 0.5421 - val_macro_f1score: 0.1557 - val_weighted_f1score: 0.0018\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 26/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.2762 - accuracy: 0.6723 - macro_f1score: 0.1951 - weighted_f1score: 0.0022\n",
      "Epoch 00026: val_loss did not improve from 2.09717\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.54212\n",
      "191/191 [==============================] - 437s 2s/step - loss: 1.2762 - accuracy: 0.6723 - macro_f1score: 0.1951 - weighted_f1score: 0.0022 - val_loss: 2.1470 - val_accuracy: 0.5370 - val_macro_f1score: 0.1565 - val_weighted_f1score: 0.0018\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 27/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.2179 - accuracy: 0.6820 - macro_f1score: 0.2027 - weighted_f1score: 0.0022\n",
      "Epoch 00027: val_loss did not improve from 2.09717\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.54212\n",
      "191/191 [==============================] - 431s 2s/step - loss: 1.2179 - accuracy: 0.6820 - macro_f1score: 0.2027 - weighted_f1score: 0.0022 - val_loss: 2.1040 - val_accuracy: 0.5387 - val_macro_f1score: 0.1577 - val_weighted_f1score: 0.0018\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 28/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.1219 - accuracy: 0.7030 - macro_f1score: 0.2117 - weighted_f1score: 0.0024\n",
      "Epoch 00028: val_loss improved from 2.09717 to 2.02973, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/028.h5\n",
      "\n",
      "Epoch 00028: val_accuracy improved from 0.54212 to 0.55944, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/028.h5\n",
      "191/191 [==============================] - 444s 2s/step - loss: 1.1219 - accuracy: 0.7030 - macro_f1score: 0.2117 - weighted_f1score: 0.0024 - val_loss: 2.0297 - val_accuracy: 0.5594 - val_macro_f1score: 0.1655 - val_weighted_f1score: 0.0019\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 29/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.0785 - accuracy: 0.7185 - macro_f1score: 0.2192 - weighted_f1score: 0.0024\n",
      "Epoch 00029: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.55944\n",
      "191/191 [==============================] - 447s 2s/step - loss: 1.0785 - accuracy: 0.7185 - macro_f1score: 0.2192 - weighted_f1score: 0.0024 - val_loss: 2.0652 - val_accuracy: 0.5577 - val_macro_f1score: 0.1715 - val_weighted_f1score: 0.0019\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 30/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 1.0152 - accuracy: 0.7359 - macro_f1score: 0.2259 - weighted_f1score: 0.0025\n",
      "Epoch 00030: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.55944\n",
      "191/191 [==============================] - 444s 2s/step - loss: 1.0152 - accuracy: 0.7359 - macro_f1score: 0.2259 - weighted_f1score: 0.0025 - val_loss: 2.0360 - val_accuracy: 0.5543 - val_macro_f1score: 0.1649 - val_weighted_f1score: 0.0019\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 31/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.9662 - accuracy: 0.7421 - macro_f1score: 0.2310 - weighted_f1score: 0.0025\n",
      "Epoch 00031: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.55944\n",
      "191/191 [==============================] - 444s 2s/step - loss: 0.9662 - accuracy: 0.7421 - macro_f1score: 0.2310 - weighted_f1score: 0.0025 - val_loss: 2.0811 - val_accuracy: 0.5567 - val_macro_f1score: 0.1692 - val_weighted_f1score: 0.0019\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 32/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.8991 - accuracy: 0.7589 - macro_f1score: 0.2398 - weighted_f1score: 0.0026\n",
      "Epoch 00032: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00032: val_accuracy improved from 0.55944 to 0.56624, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/032.h5\n",
      "191/191 [==============================] - 446s 2s/step - loss: 0.8991 - accuracy: 0.7589 - macro_f1score: 0.2398 - weighted_f1score: 0.0026 - val_loss: 2.0467 - val_accuracy: 0.5662 - val_macro_f1score: 0.1769 - val_weighted_f1score: 0.0020\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 33/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.8419 - accuracy: 0.7717 - macro_f1score: 0.2486 - weighted_f1score: 0.0027\n",
      "Epoch 00033: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.56624\n",
      "191/191 [==============================] - 435s 2s/step - loss: 0.8419 - accuracy: 0.7717 - macro_f1score: 0.2486 - weighted_f1score: 0.0027 - val_loss: 2.1353 - val_accuracy: 0.5635 - val_macro_f1score: 0.1717 - val_weighted_f1score: 0.0020\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 34/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.8217 - accuracy: 0.7760 - macro_f1score: 0.2494 - weighted_f1score: 0.0027\n",
      "Epoch 00034: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.56624\n",
      "191/191 [==============================] - 436s 2s/step - loss: 0.8217 - accuracy: 0.7760 - macro_f1score: 0.2494 - weighted_f1score: 0.0027 - val_loss: 2.0732 - val_accuracy: 0.5605 - val_macro_f1score: 0.1753 - val_weighted_f1score: 0.0020\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 35/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.7623 - accuracy: 0.7925 - macro_f1score: 0.2566 - weighted_f1score: 0.0028\n",
      "Epoch 00035: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.56624\n",
      "191/191 [==============================] - 433s 2s/step - loss: 0.7623 - accuracy: 0.7925 - macro_f1score: 0.2566 - weighted_f1score: 0.0028 - val_loss: 2.0790 - val_accuracy: 0.5656 - val_macro_f1score: 0.1716 - val_weighted_f1score: 0.0020\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 36/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.7330 - accuracy: 0.7981 - macro_f1score: 0.2609 - weighted_f1score: 0.0028\n",
      "Epoch 00036: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.56624\n",
      "191/191 [==============================] - 439s 2s/step - loss: 0.7330 - accuracy: 0.7981 - macro_f1score: 0.2609 - weighted_f1score: 0.0028 - val_loss: 2.1090 - val_accuracy: 0.5584 - val_macro_f1score: 0.1734 - val_weighted_f1score: 0.0020\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 37/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.7139 - accuracy: 0.8027 - macro_f1score: 0.2641 - weighted_f1score: 0.0029\n",
      "Epoch 00037: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00037: val_accuracy improved from 0.56624 to 0.58220, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/037.h5\n",
      "191/191 [==============================] - 445s 2s/step - loss: 0.7139 - accuracy: 0.8027 - macro_f1score: 0.2641 - weighted_f1score: 0.0029 - val_loss: 2.0390 - val_accuracy: 0.5822 - val_macro_f1score: 0.1814 - val_weighted_f1score: 0.0021\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 38/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.8484 - accuracy: 0.7675 - macro_f1score: 0.2470 - weighted_f1score: 0.0027\n",
      "Epoch 00038: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.58220\n",
      "191/191 [==============================] - 444s 2s/step - loss: 0.8484 - accuracy: 0.7675 - macro_f1score: 0.2470 - weighted_f1score: 0.0027 - val_loss: 2.1071 - val_accuracy: 0.5594 - val_macro_f1score: 0.1735 - val_weighted_f1score: 0.0020\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 39/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.6764 - accuracy: 0.8128 - macro_f1score: 0.2677 - weighted_f1score: 0.0029\n",
      "Epoch 00039: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.58220\n",
      "191/191 [==============================] - 443s 2s/step - loss: 0.6764 - accuracy: 0.8128 - macro_f1score: 0.2677 - weighted_f1score: 0.0029 - val_loss: 2.0844 - val_accuracy: 0.5724 - val_macro_f1score: 0.1779 - val_weighted_f1score: 0.0020\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 40/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.6241 - accuracy: 0.8252 - macro_f1score: 0.2765 - weighted_f1score: 0.0030\n",
      "Epoch 00040: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.58220\n",
      "191/191 [==============================] - 441s 2s/step - loss: 0.6241 - accuracy: 0.8252 - macro_f1score: 0.2765 - weighted_f1score: 0.0030 - val_loss: 2.0635 - val_accuracy: 0.5805 - val_macro_f1score: 0.1810 - val_weighted_f1score: 0.0020\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 41/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.5549 - accuracy: 0.8440 - macro_f1score: 0.2852 - weighted_f1score: 0.0031\n",
      "Epoch 00041: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00041: val_accuracy improved from 0.58220 to 0.58730, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/041.h5\n",
      "191/191 [==============================] - 446s 2s/step - loss: 0.5549 - accuracy: 0.8440 - macro_f1score: 0.2852 - weighted_f1score: 0.0031 - val_loss: 2.0884 - val_accuracy: 0.5873 - val_macro_f1score: 0.1836 - val_weighted_f1score: 0.0021\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 42/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.5371 - accuracy: 0.8495 - macro_f1score: 0.2874 - weighted_f1score: 0.0031\n",
      "Epoch 00042: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.58730\n",
      "191/191 [==============================] - 445s 2s/step - loss: 0.5371 - accuracy: 0.8495 - macro_f1score: 0.2874 - weighted_f1score: 0.0031 - val_loss: 2.1191 - val_accuracy: 0.5812 - val_macro_f1score: 0.1855 - val_weighted_f1score: 0.0021\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 43/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.4833 - accuracy: 0.8634 - macro_f1score: 0.2966 - weighted_f1score: 0.0032\n",
      "Epoch 00043: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.58730\n",
      "191/191 [==============================] - 444s 2s/step - loss: 0.4833 - accuracy: 0.8634 - macro_f1score: 0.2966 - weighted_f1score: 0.0032 - val_loss: 2.1562 - val_accuracy: 0.5825 - val_macro_f1score: 0.1863 - val_weighted_f1score: 0.0021\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 44/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.4781 - accuracy: 0.8596 - macro_f1score: 0.2980 - weighted_f1score: 0.0032\n",
      "Epoch 00044: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.58730\n",
      "191/191 [==============================] - 443s 2s/step - loss: 0.4781 - accuracy: 0.8596 - macro_f1score: 0.2980 - weighted_f1score: 0.0032 - val_loss: 2.2126 - val_accuracy: 0.5751 - val_macro_f1score: 0.1850 - val_weighted_f1score: 0.0021\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 45/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.4271 - accuracy: 0.8786 - macro_f1score: 0.3047 - weighted_f1score: 0.0033\n",
      "Epoch 00045: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.58730\n",
      "191/191 [==============================] - 443s 2s/step - loss: 0.4271 - accuracy: 0.8786 - macro_f1score: 0.3047 - weighted_f1score: 0.0033 - val_loss: 2.1169 - val_accuracy: 0.5846 - val_macro_f1score: 0.1905 - val_weighted_f1score: 0.0021\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 46/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.4337 - accuracy: 0.8764 - macro_f1score: 0.3035 - weighted_f1score: 0.0032\n",
      "Epoch 00046: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.58730\n",
      "191/191 [==============================] - 445s 2s/step - loss: 0.4337 - accuracy: 0.8764 - macro_f1score: 0.3035 - weighted_f1score: 0.0032 - val_loss: 2.1197 - val_accuracy: 0.5859 - val_macro_f1score: 0.1871 - val_weighted_f1score: 0.0021\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 47/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.4014 - accuracy: 0.8852 - macro_f1score: 0.3090 - weighted_f1score: 0.0033\n",
      "Epoch 00047: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.58730\n",
      "191/191 [==============================] - 444s 2s/step - loss: 0.4014 - accuracy: 0.8852 - macro_f1score: 0.3090 - weighted_f1score: 0.0033 - val_loss: 2.1588 - val_accuracy: 0.5832 - val_macro_f1score: 0.1921 - val_weighted_f1score: 0.0021\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 48/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.4231 - accuracy: 0.8765 - macro_f1score: 0.3041 - weighted_f1score: 0.0033\n",
      "Epoch 00048: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.58730\n",
      "191/191 [==============================] - 443s 2s/step - loss: 0.4231 - accuracy: 0.8765 - macro_f1score: 0.3041 - weighted_f1score: 0.0033 - val_loss: 2.1525 - val_accuracy: 0.5781 - val_macro_f1score: 0.1911 - val_weighted_f1score: 0.0021\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 49/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.3892 - accuracy: 0.8856 - macro_f1score: 0.3103 - weighted_f1score: 0.0033\n",
      "Epoch 00049: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.58730\n",
      "191/191 [==============================] - 446s 2s/step - loss: 0.3892 - accuracy: 0.8856 - macro_f1score: 0.3103 - weighted_f1score: 0.0033 - val_loss: 2.1872 - val_accuracy: 0.5853 - val_macro_f1score: 0.1909 - val_weighted_f1score: 0.0021\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 50/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.3630 - accuracy: 0.8960 - macro_f1score: 0.3150 - weighted_f1score: 0.0033\n",
      "Epoch 00050: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.58730\n",
      "191/191 [==============================] - 444s 2s/step - loss: 0.3630 - accuracy: 0.8960 - macro_f1score: 0.3150 - weighted_f1score: 0.0033 - val_loss: 2.1753 - val_accuracy: 0.5846 - val_macro_f1score: 0.1882 - val_weighted_f1score: 0.0021\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 51/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.1129 - accuracy: 0.9739 - macro_f1score: 0.3570 - weighted_f1score: 0.0037\n",
      "Epoch 00051: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00051: val_accuracy improved from 0.58730 to 0.59579, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/051.h5\n",
      "191/191 [==============================] - 445s 2s/step - loss: 0.1129 - accuracy: 0.9739 - macro_f1score: 0.3570 - weighted_f1score: 0.0037 - val_loss: 2.2032 - val_accuracy: 0.5958 - val_macro_f1score: 0.1970 - val_weighted_f1score: 0.0022\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 52/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9839 - macro_f1score: 0.3617 - weighted_f1score: 0.0038\n",
      "Epoch 00052: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00052: val_accuracy improved from 0.59579 to 0.60122, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/052.h5\n",
      "191/191 [==============================] - 448s 2s/step - loss: 0.0754 - accuracy: 0.9839 - macro_f1score: 0.3617 - weighted_f1score: 0.0038 - val_loss: 2.2758 - val_accuracy: 0.6012 - val_macro_f1score: 0.1969 - val_weighted_f1score: 0.0022\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 53/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9847 - macro_f1score: 0.3641 - weighted_f1score: 0.0038\n",
      "Epoch 00053: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00053: val_accuracy improved from 0.60122 to 0.60156, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/053.h5\n",
      "191/191 [==============================] - 447s 2s/step - loss: 0.0654 - accuracy: 0.9847 - macro_f1score: 0.3641 - weighted_f1score: 0.0038 - val_loss: 2.3289 - val_accuracy: 0.6016 - val_macro_f1score: 0.2008 - val_weighted_f1score: 0.0023\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 54/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9873 - macro_f1score: 0.3654 - weighted_f1score: 0.0038\n",
      "Epoch 00054: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00054: val_accuracy improved from 0.60156 to 0.60360, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/054.h5\n",
      "191/191 [==============================] - 446s 2s/step - loss: 0.0588 - accuracy: 0.9873 - macro_f1score: 0.3654 - weighted_f1score: 0.0038 - val_loss: 2.3676 - val_accuracy: 0.6036 - val_macro_f1score: 0.1976 - val_weighted_f1score: 0.0023\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 55/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9897 - macro_f1score: 0.3663 - weighted_f1score: 0.0038\n",
      "Epoch 00055: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.60360\n",
      "191/191 [==============================] - 444s 2s/step - loss: 0.0505 - accuracy: 0.9897 - macro_f1score: 0.3663 - weighted_f1score: 0.0038 - val_loss: 2.3906 - val_accuracy: 0.6026 - val_macro_f1score: 0.2002 - val_weighted_f1score: 0.0022\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 56/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.0499 - accuracy: 0.9891 - macro_f1score: 0.3665 - weighted_f1score: 0.0038\n",
      "Epoch 00056: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.60360\n",
      "191/191 [==============================] - 443s 2s/step - loss: 0.0499 - accuracy: 0.9891 - macro_f1score: 0.3665 - weighted_f1score: 0.0038 - val_loss: 2.4164 - val_accuracy: 0.6022 - val_macro_f1score: 0.1975 - val_weighted_f1score: 0.0023\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 57/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9912 - macro_f1score: 0.3666 - weighted_f1score: 0.0038\n",
      "Epoch 00057: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.60360\n",
      "191/191 [==============================] - 442s 2s/step - loss: 0.0418 - accuracy: 0.9912 - macro_f1score: 0.3666 - weighted_f1score: 0.0038 - val_loss: 2.4501 - val_accuracy: 0.6019 - val_macro_f1score: 0.2021 - val_weighted_f1score: 0.0023\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 58/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9918 - macro_f1score: 0.3663 - weighted_f1score: 0.0039\n",
      "Epoch 00058: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.60360\n",
      "191/191 [==============================] - 445s 2s/step - loss: 0.0373 - accuracy: 0.9918 - macro_f1score: 0.3663 - weighted_f1score: 0.0039 - val_loss: 2.5087 - val_accuracy: 0.6036 - val_macro_f1score: 0.2061 - val_weighted_f1score: 0.0023\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 59/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.0393 - accuracy: 0.9913 - macro_f1score: 0.3664 - weighted_f1score: 0.0038\n",
      "Epoch 00059: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00059: val_accuracy improved from 0.60360 to 0.60802, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/ResNet50/059.h5\n",
      "191/191 [==============================] - 444s 2s/step - loss: 0.0393 - accuracy: 0.9913 - macro_f1score: 0.3664 - weighted_f1score: 0.0038 - val_loss: 2.4653 - val_accuracy: 0.6080 - val_macro_f1score: 0.2029 - val_weighted_f1score: 0.0023\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 60/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9919 - macro_f1score: 0.3669 - weighted_f1score: 0.0039\n",
      "Epoch 00060: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.60802\n",
      "191/191 [==============================] - 444s 2s/step - loss: 0.0387 - accuracy: 0.9919 - macro_f1score: 0.3669 - weighted_f1score: 0.0039 - val_loss: 2.5035 - val_accuracy: 0.5992 - val_macro_f1score: 0.2005 - val_weighted_f1score: 0.0023\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 61/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9936 - macro_f1score: 0.3685 - weighted_f1score: 0.0039\n",
      "Epoch 00061: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.60802\n",
      "191/191 [==============================] - 445s 2s/step - loss: 0.0309 - accuracy: 0.9936 - macro_f1score: 0.3685 - weighted_f1score: 0.0039 - val_loss: 2.5498 - val_accuracy: 0.6043 - val_macro_f1score: 0.2042 - val_weighted_f1score: 0.0023\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 62/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9929 - macro_f1score: 0.3670 - weighted_f1score: 0.0039\n",
      "Epoch 00062: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.60802\n",
      "191/191 [==============================] - 447s 2s/step - loss: 0.0334 - accuracy: 0.9929 - macro_f1score: 0.3670 - weighted_f1score: 0.0039 - val_loss: 2.5631 - val_accuracy: 0.5988 - val_macro_f1score: 0.2007 - val_weighted_f1score: 0.0023\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 63/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9922 - macro_f1score: 0.3690 - weighted_f1score: 0.0039\n",
      "Epoch 00063: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.60802\n",
      "191/191 [==============================] - 448s 2s/step - loss: 0.0324 - accuracy: 0.9922 - macro_f1score: 0.3690 - weighted_f1score: 0.0039 - val_loss: 2.5876 - val_accuracy: 0.6029 - val_macro_f1score: 0.2023 - val_weighted_f1score: 0.0023\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 64/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9930 - macro_f1score: 0.3670 - weighted_f1score: 0.0039\n",
      "Epoch 00064: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.60802\n",
      "191/191 [==============================] - 446s 2s/step - loss: 0.0324 - accuracy: 0.9930 - macro_f1score: 0.3670 - weighted_f1score: 0.0039 - val_loss: 2.5737 - val_accuracy: 0.6029 - val_macro_f1score: 0.2037 - val_weighted_f1score: 0.0023\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 65/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9934 - macro_f1score: 0.3677 - weighted_f1score: 0.0039\n",
      "Epoch 00065: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.60802\n",
      "191/191 [==============================] - 445s 2s/step - loss: 0.0313 - accuracy: 0.9934 - macro_f1score: 0.3677 - weighted_f1score: 0.0039 - val_loss: 2.6299 - val_accuracy: 0.5968 - val_macro_f1score: 0.1992 - val_weighted_f1score: 0.0023\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 66/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9944 - macro_f1score: 0.3665 - weighted_f1score: 0.0039\n",
      "Epoch 00066: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.60802\n",
      "191/191 [==============================] - 443s 2s/step - loss: 0.0264 - accuracy: 0.9944 - macro_f1score: 0.3665 - weighted_f1score: 0.0039 - val_loss: 2.7028 - val_accuracy: 0.5968 - val_macro_f1score: 0.2054 - val_weighted_f1score: 0.0023\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 67/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9935 - macro_f1score: 0.3673 - weighted_f1score: 0.0039\n",
      "Epoch 00067: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.60802\n",
      "191/191 [==============================] - 443s 2s/step - loss: 0.0291 - accuracy: 0.9935 - macro_f1score: 0.3673 - weighted_f1score: 0.0039 - val_loss: 2.6743 - val_accuracy: 0.5982 - val_macro_f1score: 0.2069 - val_weighted_f1score: 0.0023\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 68/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9946 - macro_f1score: 0.3681 - weighted_f1score: 0.0039\n",
      "Epoch 00068: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.60802\n",
      "191/191 [==============================] - 443s 2s/step - loss: 0.0254 - accuracy: 0.9946 - macro_f1score: 0.3681 - weighted_f1score: 0.0039 - val_loss: 2.6568 - val_accuracy: 0.5958 - val_macro_f1score: 0.2004 - val_weighted_f1score: 0.0023\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 69/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9942 - macro_f1score: 0.3683 - weighted_f1score: 0.0039\n",
      "Epoch 00069: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.60802\n",
      "191/191 [==============================] - 445s 2s/step - loss: 0.0252 - accuracy: 0.9942 - macro_f1score: 0.3683 - weighted_f1score: 0.0039 - val_loss: 2.6561 - val_accuracy: 0.6060 - val_macro_f1score: 0.2028 - val_weighted_f1score: 0.0023\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 70/70\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9939 - macro_f1score: 0.3674 - weighted_f1score: 0.0039\n",
      "Epoch 00070: val_loss did not improve from 2.02973\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.60802\n",
      "191/191 [==============================] - 444s 2s/step - loss: 0.0271 - accuracy: 0.9939 - macro_f1score: 0.3674 - weighted_f1score: 0.0039 - val_loss: 2.7049 - val_accuracy: 0.6019 - val_macro_f1score: 0.2049 - val_weighted_f1score: 0.0023\n"
     ]
    }
   ],
   "source": [
    "######## flow_from_directory\n",
    "history = model.fit(train_generator, steps_per_epoch=int(len(x_train)/batch_sizes),  validation_data = valid_generator, epochs=epochs , verbose=1 , callbacks = callbacks_list , validation_steps=int(len(x_valid)/batch_sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GrcVLA1ARMTB"
   },
   "source": [
    "### 2) ResNet50 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 45717264,
     "status": "ok",
     "timestamp": 1599359326958,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "07323071725004325774"
     },
     "user_tz": -540
    },
    "id": "l2YDIRWJRMTC",
    "outputId": "7ee8fa19-f3de-4bb1-cf14-1c8eeff8ae9e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1130s 47s/step - loss: 2.7629 - accuracy: 0.5791 - macro_f1score: 0.1970 - weighted_f1score: 0.0022\n",
      "[Test Loss: 2.7629 /  Test Accuracy: 0.5791 / Test Macro f1: 0.1970 / Test Weighted f1: 0.0022]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. epoch=maximum\n",
    "loss , acc, mf1, wf1 = model.evaluate(test_generator,steps=int(len(x_test)/batch_sizes))\n",
    "print('[Test Loss: %.4f /  Test Accuracy: %.4f / Test Macro f1: %.4f / Test Weighted f1: %.4f]\\n' % (loss,acc,mf1,wf1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QdFa0ZaJRMTF"
   },
   "outputs": [],
   "source": [
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "acc=history.history['accuracy']\n",
    "val_acc=history.history['val_accuracy']\n",
    "f1=history.history['macro_f1score']\n",
    "val_f1=history.history['val_macro_f1score']\n",
    "epochs=range(1,len(acc)+1)\n",
    "\n",
    "data = np.array([epochs,loss,val_loss,acc,val_acc,f1,val_f1]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XF_NRyU7RMTL"
   },
   "outputs": [],
   "source": [
    "# data save\n",
    "# epochs, loss, val_loss, acc, val_acc, f1, val_f1\n",
    "\n",
    "np.savetxt(os.path.join(dir,'train_valid_output',number,model.name+'.txt'),data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UxWVS1nWRMTP"
   },
   "outputs": [],
   "source": [
    "# data import\n",
    "data = np.loadtxt(os.path.join(dir,'train_valid_output',number,'ResNet50.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZVGF8yllRMTT"
   },
   "outputs": [],
   "source": [
    "epochs=data[:,0]\n",
    "loss=data[:,1]\n",
    "val_loss=data[:,2]\n",
    "acc=data[:,3]\n",
    "val_acc=data[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 45718061,
     "status": "ok",
     "timestamp": 1599359327809,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "07323071725004325774"
     },
     "user_tz": -540
    },
    "id": "QaWMbe_URMTW",
    "outputId": "a63f4b18-ce2c-44da-83dd-fac9ef7e10e1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs[1:],acc[1:],'b',label='Training Acc')\n",
    "plt.plot(epochs[1:],val_acc[1:],'r',label='Validation Acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs[10:],loss[10:],'b',label='Training Loss')\n",
    "plt.plot(epochs[10:],val_loss[10:],'r',label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OnXT7xiED9y3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i-O7A50sD_cH"
   },
   "outputs": [],
   "source": [
    "# 이 아래는 내가 인위적으로 살펴보고 싶은 epoch에 대해서 결과값 출력 / resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model(os.path.join(dir,'model_output',number,'ResNet50','022.h5'),custom_objects={\"macro_f1score\": macro_f1score,\"weighted_f1score\":weighted_f1score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FQo7NZqoD_cM"
   },
   "outputs": [],
   "source": [
    "# 2. epoch=?\n",
    "loss , acc, mf1, wf1 = model.evaluate(test_generator,steps=int(len(x_test)/batch_sizes))\n",
    "print('[Test Loss: %.4f /  Test Accuracy: %.4f / Test Macro f1: %.4f / Test Weighted f1: %.4f]\\n' % (loss,acc,mf1,wf1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V0smI--HD9xA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L29NG_c6EAEo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "5.ResNet50.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
