{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ucbQyskZRiX_"
   },
   "outputs": [],
   "source": [
    "### 참고 :  https://sike6054.github.io/blog/paper/sixth-post/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vwVmRuFcUBkT"
   },
   "source": [
    "# [DenseNet121]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0MMz6DhUBkW"
   },
   "source": [
    "*KU LeeDongGyu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ywk25Fr79dWe"
   },
   "source": [
    "## Contents\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KSON5xiR9dWf"
   },
   "source": [
    "1. Data Preprocessing\n",
    "```\n",
    "1) Data Import\n",
    "2) Data Augmentation\n",
    "```\n",
    "2. Support Functions & Almost Original DenseNet121\n",
    "```\n",
    "1) Support Functions\n",
    "2) Almost orginal DenseNet121\n",
    "```\n",
    "3. DenseNet121\n",
    "```\n",
    "1) DenseNet121\n",
    "2) DenseNet121 Evaluate\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U01q4o40UBkY"
   },
   "source": [
    "### Install Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XjdygsS_UBke"
   },
   "source": [
    "### Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 77531,
     "status": "ok",
     "timestamp": 1599313691385,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "03303793760957673272"
     },
     "user_tz": -540
    },
    "id": "o1tpIlBhXG2i",
    "outputId": "1d9262d0-5332-4245-87a1-b81980a7cf53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 77512,
     "status": "ok",
     "timestamp": 1599313691388,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "03303793760957673272"
     },
     "user_tz": -540
    },
    "id": "IJdbC2nRXR6h",
    "outputId": "f50a47bc-8981-4c5d-9be2-8e2978d3e8d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/Paper\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/My Drive/Colab Notebooks/Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I98omoQ8FF90"
   },
   "outputs": [],
   "source": [
    "from f1score import macro_f1score,weighted_f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKLMWqbuUBkf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as ks\n",
    "from tensorflow.keras import backend as K \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input, Concatenate, ZeroPadding2D ,GlobalMaxPooling2D, Reshape , Lambda , Add, Multiply\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, BatchNormalization, AveragePooling2D , ZeroPadding2D, SeparableConv2D\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop , SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping , LearningRateScheduler, ModelCheckpoint, CSVLogger, Callback, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l1,l2,l1_l2\n",
    "from tensorflow.keras.models import Model , load_model , Sequential\n",
    "from tensorflow.keras.utils import plot_model , to_categorical, get_file\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 81163,
     "status": "ok",
     "timestamp": 1599313695093,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "03303793760957673272"
     },
     "user_tz": -540
    },
    "id": "cbiFovMgXTax",
    "outputId": "19bf7828-02ce-4aa0-af8c-a4689183cf6e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/My Drive/Colab Notebooks/Paper'"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 86464,
     "status": "ok",
     "timestamp": 1599313700420,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "03303793760957673272"
     },
     "user_tz": -540
    },
    "id": "AcT0A_iwEzaZ",
    "outputId": "a882efcf-91f1-4ef2-ca8b-d816332ab61d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 86832,
     "status": "ok",
     "timestamp": 1599313700813,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "03303793760957673272"
     },
     "user_tz": -540
    },
    "id": "Gr5hMHvmABmG",
    "outputId": "c4bb587f-c3dd-4fb0-a8b6-b8a7881e82cd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 86778,
     "status": "ok",
     "timestamp": 1599313700814,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "03303793760957673272"
     },
     "user_tz": -540
    },
    "id": "Kf057Rw2yigs",
    "outputId": "5c823f8f-4dc1-4e4b-ff3e-a52c75aced91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13109313769967077069\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 7624768429943792888\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 17099049296186962441\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15473775744\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12414077251619341910\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sTXnS6_magkg"
   },
   "source": [
    "## 1. Data Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FWigHOuzCRRj"
   },
   "source": [
    "### 1) Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BeJ7UtuOEgWY"
   },
   "outputs": [],
   "source": [
    "# 바꿔서 살펴 볼 것들\n",
    "# CALTECH, CIFAR100, FER, MIT\n",
    "data_name = 'CALTECH'\n",
    "gan_type = 'No_GAN'\n",
    "number = '1'\n",
    "size = 224 # sizes after cropping\n",
    "super_size = 256 # sizes before cropping \n",
    "input_sizes = (size,size,3)\n",
    "batch_sizes = 64\n",
    "weight_decay = 4e-5\n",
    "epochs = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SPnWxfGzLOF6"
   },
   "outputs": [],
   "source": [
    "# 참고 : https://stackoverflow.com/questions/32419510/how-to-get-reproducible-results-in-keras/52897216#52897216\n",
    "# setting the seed number for random number generation for reproducibility.\n",
    "\n",
    "from numpy.random import seed\n",
    "import random\n",
    "\n",
    "\n",
    "if number=='1':\n",
    "    seed_num = 200225\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_num)\n",
    "    random.seed(seed_num)\n",
    "    seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "elif number=='2':\n",
    "    seed_num = 727\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_num)\n",
    "    random.seed(seed_num)\n",
    "    seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "elif number=='3':\n",
    "    seed_num = 115\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_num)\n",
    "    random.seed(seed_num)\n",
    "    seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "elif number=='4':\n",
    "    seed_num = 501\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_num)\n",
    "    random.seed(seed_num)\n",
    "    seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "elif number=='5':\n",
    "    seed_num = 517\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_num)\n",
    "    random.seed(seed_num)\n",
    "    seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AbP99pIGoIUp"
   },
   "outputs": [],
   "source": [
    "# data import\n",
    "\n",
    "if data_name=='FER' :\n",
    "    x_train =  np.zeros(28698)\n",
    "    x_valid = np.zeros(3589)\n",
    "    x_test = np.zeros(3588)\n",
    "    classes = 7 \n",
    "    tr_center = [0.50793296, 0.50793296, 0.50793296]\n",
    "elif data_name=='MIT':\n",
    "    x_train = np.zeros(12466)\n",
    "    x_valid = np.zeros(1564)\n",
    "    x_test = np.zeros(1590)\n",
    "    classes = 67 \n",
    "    tr_center = [0.47916578, 0.42029615, 0.36046057]\n",
    "elif data_name=='CALTECH':\n",
    "    x_train = np.zeros(24510)\n",
    "    x_valid = np.zeros(2980)\n",
    "    x_test = np.zeros(3118)\n",
    "    classes = 257\n",
    "    tr_center = [0.51397761, 0.49525248, 0.46555727]\n",
    "elif data_name=='CIFAR100':\n",
    "    x_train = np.zeros(39941)\n",
    "    x_valid = np.zeros(10059)\n",
    "    x_test = np.zeros(10000)\n",
    "    classes = 100\n",
    "    tr_center = [0.53393271, 0.51324147, 0.46450563]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQlStjHWt-jM"
   },
   "outputs": [],
   "source": [
    "dir = os.path.join(os.getcwd(),data_name,gan_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wzhKX9OVoIUw"
   },
   "source": [
    "### 2) Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iz9u1paNoIUx"
   },
   "outputs": [],
   "source": [
    "# 참고 : https://jkjung-avt.github.io/keras-image-cropping/\n",
    "\n",
    "def random_crop(img, random_crop_size):\n",
    "    # Note: image_data_format is 'channel_last'\n",
    "    assert img.shape[2] == 3 # img.shape[2] 가 3(rgb)이 아니면 assertion error 발생\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    dy, dx = random_crop_size\n",
    "    x = np.random.randint(0, width - dx + 1)\n",
    "    y = np.random.randint(0, height - dy + 1)\n",
    "    return img[y:(y+dy), x:(x+dx), :]\n",
    "\n",
    "\n",
    "def crop_generator(batches, crop_length):\n",
    "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
    "    crops from the image batches generated by the original iterator.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
    "        yield (batch_crops, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EW0KZ6x9EBtf"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array,load_img\n",
    "import glob\n",
    "\n",
    "# 데이터 전체에 대해 centering 진행함.\n",
    "\n",
    "def read_cal_image(img_path): \n",
    "    x = img_to_array(load_img(img_path)) # x는 채널별 평균값\n",
    "    y = x.shape[0] * x.shape[1]# y는 데이터별 픽셀 수 (비중)\n",
    "\n",
    "    x = 1/255. * x # scaling하고, centering값을 뽑아냄.\n",
    "    x = np.mean(x, axis=(0,1))\n",
    "    \n",
    "    return np.hstack([x,y])\n",
    "\n",
    "def calculate_centered_mean(dataset_path,x_train=x_train):\n",
    "    num = len(x_train)\n",
    "    space = np.empty((num,4))\n",
    "    i=0\n",
    "\n",
    "    for p in glob.glob(os.path.join(dataset_path,'*/*.*')) :\n",
    "        space[i] = read_cal_image(p)\n",
    "        i += 1\n",
    "\n",
    "    ratio = space[:,3] / np.sum(space[:,3])\n",
    "\n",
    "    return np.average(space[:,0:3],axis=0,weights=ratio)\n",
    "\n",
    "\n",
    "# 아래의 함수를 돌려서 나온 결과값을 중심화 값으로 설정.\n",
    "\n",
    "# train_mean = calculate_centered_mean(os.path.join(dir,'data/train')).reshape((1,1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c2IcuMY2oIU3"
   },
   "outputs": [],
   "source": [
    "datagen_tr = ImageDataGenerator(\n",
    "    rescale=1/255.,\n",
    "    horizontal_flip=True,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=False,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=[0.9,1.0],\n",
    "    fill_mode = 'nearest')\n",
    "datagen_val = ImageDataGenerator(rescale=1/255.,featurewise_center=True)\n",
    "datagen_tes = ImageDataGenerator(rescale=1/255.,featurewise_center=True)\n",
    "\n",
    "# 원래는 이 자리에 fit 매서드를 써야하지만, 그냥 내가 중심화함수를 만들고 적용함. \n",
    "\n",
    "# 중심화 설정\n",
    "datagen_tr.mean = np.array(tr_center, dtype=np.float32).reshape((1,1,3)) # RGB\n",
    "datagen_val.mean = np.array(tr_center, dtype=np.float32).reshape((1,1,3)) # RGB\n",
    "datagen_tes.mean = np.array(tr_center, dtype=np.float32).reshape((1,1,3)) # RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 148176,
     "status": "ok",
     "timestamp": 1599313762356,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "03303793760957673272"
     },
     "user_tz": -540
    },
    "id": "4t6c_JewoIU7",
    "outputId": "f1cb1e4a-e954-4ef5-d78e-92c6a04098fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24509 images belonging to 257 classes.\n",
      "Found 2980 images belonging to 257 classes.\n",
      "Found 3118 images belonging to 257 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = datagen_tr.flow_from_directory(directory=os.path.join(dir,'data/train'),target_size=(super_size,super_size),batch_size=batch_sizes,class_mode='categorical') # fer : 28698 / mit : 12466 / caltech : 24509 / cifar : 39941\n",
    "train_generator= crop_generator(train_batches, size)\n",
    "valid_generator = datagen_val.flow_from_directory(directory=os.path.join(dir,'data/valid'),target_size=(size,size),batch_size=batch_sizes,class_mode='categorical') # fer : 3589 / mit : 1564 / caltech : 2980 / cifar : 10059\n",
    "test_generator = datagen_tes.flow_from_directory(directory=os.path.join(dir,'data/test'),target_size=(size,size),batch_size=batch_sizes,class_mode='categorical') # fer : 3588 / mit : 1590 / caltech : 3118 / cifar : 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gn_DwVVV1qmT"
   },
   "source": [
    "## 2. Support Functions & Almost Original DenseNet121\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GrcgSgWh1qmT"
   },
   "source": [
    "### 1) Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XA3sqFv_ZyBS"
   },
   "outputs": [],
   "source": [
    "# def lr_schedule(epoch):\n",
    "#     init_lr = 1e-4\n",
    "#     k = 0.04\n",
    "#     lr = init_lr * np.exp(-k*epoch)\n",
    "#     print('Learning rate: ', lr)\n",
    "#     return lr\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch < 60:\n",
    "        lr = lr\n",
    "    else :\n",
    "        lr = lr * 0.1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uY2LEhqW1qmX"
   },
   "outputs": [],
   "source": [
    "def Conv_Block(x, growth_rate, activation='relu', weight_decay=weight_decay):\n",
    "    x_l = BatchNormalization()(x)\n",
    "    x_l = Activation(activation)(x_l)\n",
    "    x_l = Conv2D(growth_rate*4, (1, 1), padding='same', kernel_initializer='he_uniform', kernel_regularizer=l2(weight_decay))(x_l)\n",
    "    \n",
    "    x_l = BatchNormalization()(x_l)\n",
    "    x_l = Activation(activation)(x_l)\n",
    "    x_l = Conv2D(growth_rate, (3, 3), padding='same', kernel_initializer='he_uniform', kernel_regularizer=l2(weight_decay))(x_l)\n",
    "    \n",
    "    x = Concatenate()([x, x_l])\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VOb3c3CuirRq"
   },
   "outputs": [],
   "source": [
    "def Dense_Block(x, layers, growth_rate):\n",
    "    for i in range(layers):\n",
    "        x = Conv_Block(x, growth_rate)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ml3iqHkrirV-"
   },
   "outputs": [],
   "source": [
    "def Transition_Layer(x, compression_factor=0.5, activation='relu', weight_decay=weight_decay):\n",
    "    reduced_filters = int(K.int_shape(x)[-1] * compression_factor)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    x = Conv2D(reduced_filters, (1, 1), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(x)\n",
    "    \n",
    "    x = AveragePooling2D((2, 2), padding='same', strides=2)(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Lho4zoe1qmc"
   },
   "source": [
    "### 2) Almost Orginial DenseNet121\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5IqNGAIi1qmZ"
   },
   "outputs": [],
   "source": [
    "def DenseNet(input_shape, classes, weight_decay=weight_decay, growth_rate = 32, densenet_type='DenseNet_121', name = 'DenseNet121'):\n",
    "\n",
    "    model_input = Input(shape=input_shape)\n",
    "    layers_in_block = {'DenseNet_121' : [6, 12, 24, 16],\n",
    "                    'DenseNet_169' : [6, 12, 32, 32],\n",
    "                    'DenseNet_201' : [6, 12, 48, 32],\n",
    "                    'DenseNet_265' : [6, 12, 64, 48]}\n",
    "\n",
    "    x = Conv2D(growth_rate*2, (7, 7), padding='same', strides=2, kernel_initializer='he_uniform', kernel_regularizer=l2(weight_decay))(model_input) # (224, 224, 3) -> (112, 112, 64)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), padding='same', strides=2)(x) # (112, 112, 64) -> (56, 56, 64)\n",
    "\n",
    "    x = Dense_Block(x, layers_in_block[densenet_type][0], growth_rate)\n",
    "    x = Transition_Layer(x)\n",
    "    x = Dense_Block(x, layers_in_block[densenet_type][1], growth_rate)\n",
    "    x = Transition_Layer(x)\n",
    "    x = Dense_Block(x, layers_in_block[densenet_type][2], growth_rate)\n",
    "    x = Transition_Layer(x)\n",
    "    x = Dense_Block(x, layers_in_block[densenet_type][3], growth_rate)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    model_output = Dense(classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(model_input, model_output, name=name)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Y24Va7X1qmf"
   },
   "source": [
    "## 3. DenseNet121\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wiva9k1w1qmg"
   },
   "source": [
    "### 1) DenseNet121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WeiAeNmA1qmg"
   },
   "outputs": [],
   "source": [
    "# densenet\n",
    "\n",
    "model = DenseNet(input_shape=input_sizes, classes = classes, growth_rate = 32, densenet_type='DenseNet_121', name = 'DenseNet121')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 151918,
     "status": "ok",
     "timestamp": 1599313766202,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "03303793760957673272"
     },
     "user_tz": -540
    },
    "id": "5w9jMQqhN1pv",
    "outputId": "fd192a8c-6bbb-4bc8-856e-9a0e4c9b98a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DenseNet121\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 112, 112, 64) 9472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 112, 112, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 112, 112, 64) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 56, 56, 64)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 56, 56, 64)   256         max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 56, 56, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 56, 56, 128)  8320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 56, 56, 128)  512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 56, 56, 128)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 56, 56, 32)   36896       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 56, 56, 96)   0           max_pooling2d[0][0]              \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 56, 56, 96)   384         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 56, 56, 96)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 56, 56, 128)  12416       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 56, 56, 128)  512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 56, 56, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 56, 56, 32)   36896       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 56, 56, 128)  0           concatenate[0][0]                \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 56, 56, 128)  512         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 56, 56, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 56, 56, 128)  16512       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 56, 56, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 56, 56, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 56, 56, 32)   36896       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 56, 56, 160)  0           concatenate_1[0][0]              \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 56, 56, 160)  640         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 56, 56, 160)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 56, 56, 128)  20608       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 56, 56, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 56, 56, 128)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 56, 56, 32)   36896       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 56, 56, 192)  0           concatenate_2[0][0]              \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 56, 56, 192)  768         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 56, 56, 192)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 56, 56, 128)  24704       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 56, 56, 128)  512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 56, 56, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 56, 56, 32)   36896       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 56, 56, 224)  0           concatenate_3[0][0]              \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 56, 56, 224)  896         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 56, 56, 224)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 56, 56, 128)  28800       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 56, 56, 128)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 56, 56, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 56, 56, 32)   36896       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 56, 56, 256)  0           concatenate_4[0][0]              \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 56, 56, 256)  1024        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 56, 56, 256)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 56, 56, 128)  32896       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 28, 28, 128)  0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 28, 28, 128)  512         average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 28, 28, 128)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 28, 28, 128)  16512       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 28, 28, 128)  512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 28, 28, 128)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 28, 28, 32)   36896       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 28, 28, 160)  0           average_pooling2d[0][0]          \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 28, 28, 160)  640         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 28, 28, 160)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 28, 28, 128)  20608       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 28, 28, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 28, 28, 128)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 28, 28, 32)   36896       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 28, 28, 192)  0           concatenate_6[0][0]              \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 28, 28, 192)  768         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 28, 28, 192)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 28, 28, 128)  24704       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 28, 28, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 28, 28, 128)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 28, 28, 32)   36896       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 28, 28, 224)  0           concatenate_7[0][0]              \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 28, 28, 224)  896         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 28, 28, 224)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 28, 28, 128)  28800       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 28, 28, 128)  512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 28, 28, 128)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 28, 28, 32)   36896       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 28, 28, 256)  0           concatenate_8[0][0]              \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 28, 28, 256)  1024        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 28, 28, 256)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 28, 28, 128)  32896       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 28, 28, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 28, 28, 128)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 28, 28, 32)   36896       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 28, 28, 288)  0           concatenate_9[0][0]              \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 28, 28, 288)  1152        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 28, 28, 288)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 28, 28, 128)  36992       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 28, 28, 128)  512         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 28, 28, 128)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 28, 28, 32)   36896       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 28, 28, 320)  0           concatenate_10[0][0]             \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 28, 28, 320)  1280        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 28, 28, 320)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 28, 28, 128)  41088       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 28, 28, 128)  512         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 28, 28, 128)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 28, 28, 32)   36896       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 28, 28, 352)  0           concatenate_11[0][0]             \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 28, 28, 352)  1408        concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 28, 28, 352)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 28, 28, 128)  45184       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 28, 28, 128)  512         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 28, 28, 128)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 28, 28, 32)   36896       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 28, 28, 384)  0           concatenate_12[0][0]             \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 28, 28, 384)  1536        concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 28, 28, 384)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 28, 28, 128)  49280       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 28, 28, 128)  512         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 28, 28, 128)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 28, 28, 32)   36896       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 28, 28, 416)  0           concatenate_13[0][0]             \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 28, 28, 416)  1664        concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 28, 28, 416)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 28, 28, 128)  53376       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 28, 28, 128)  512         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 28, 28, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 28, 28, 32)   36896       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 28, 28, 448)  0           concatenate_14[0][0]             \n",
      "                                                                 conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 28, 28, 448)  1792        concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 28, 28, 448)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 28, 28, 128)  57472       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 28, 28, 128)  512         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 28, 28, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 28, 28, 32)   36896       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 28, 28, 480)  0           concatenate_15[0][0]             \n",
      "                                                                 conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 28, 28, 480)  1920        concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 28, 28, 480)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 28, 28, 128)  61568       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 28, 28, 128)  512         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 28, 28, 128)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 28, 28, 32)   36896       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 28, 28, 512)  0           concatenate_16[0][0]             \n",
      "                                                                 conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 28, 28, 512)  2048        concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 28, 28, 512)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 28, 28, 256)  131328      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 14, 14, 256)  0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 14, 14, 256)  1024        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 14, 14, 256)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 14, 14, 128)  32896       activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 14, 14, 128)  512         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 14, 14, 128)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 14, 14, 32)   36896       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 14, 14, 288)  0           average_pooling2d_1[0][0]        \n",
      "                                                                 conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 14, 14, 288)  1152        concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 14, 14, 288)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 14, 14, 128)  36992       activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 14, 14, 128)  512         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 14, 14, 128)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 14, 14, 32)   36896       activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 14, 14, 320)  0           concatenate_18[0][0]             \n",
      "                                                                 conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 14, 14, 320)  1280        concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 14, 14, 320)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 14, 14, 128)  41088       activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 14, 14, 128)  512         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 14, 14, 128)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 14, 14, 32)   36896       activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 14, 14, 352)  0           concatenate_19[0][0]             \n",
      "                                                                 conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 14, 14, 352)  1408        concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 14, 14, 352)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 14, 14, 128)  45184       activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 14, 14, 128)  512         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 14, 14, 128)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 14, 14, 32)   36896       activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 14, 14, 384)  0           concatenate_20[0][0]             \n",
      "                                                                 conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 14, 14, 384)  1536        concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 14, 14, 384)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 14, 14, 128)  49280       activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 14, 14, 128)  512         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 14, 14, 128)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 14, 14, 32)   36896       activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 14, 14, 416)  0           concatenate_21[0][0]             \n",
      "                                                                 conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 14, 14, 416)  1664        concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 14, 14, 416)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 14, 14, 128)  53376       activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 14, 14, 128)  512         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 14, 14, 128)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 14, 14, 32)   36896       activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 14, 14, 448)  0           concatenate_22[0][0]             \n",
      "                                                                 conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 14, 14, 448)  1792        concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 14, 14, 448)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 14, 14, 128)  57472       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 14, 14, 128)  512         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 14, 14, 128)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 14, 14, 32)   36896       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 14, 14, 480)  0           concatenate_23[0][0]             \n",
      "                                                                 conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 14, 14, 480)  1920        concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 14, 14, 480)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 14, 14, 128)  61568       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 14, 14, 128)  512         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 14, 14, 128)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 14, 14, 32)   36896       activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 14, 14, 512)  0           concatenate_24[0][0]             \n",
      "                                                                 conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 14, 14, 512)  2048        concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 14, 14, 512)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 14, 14, 128)  65664       activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 14, 14, 128)  512         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 14, 14, 128)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 14, 14, 32)   36896       activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 14, 14, 544)  0           concatenate_25[0][0]             \n",
      "                                                                 conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 14, 14, 544)  2176        concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 14, 14, 544)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 14, 14, 128)  69760       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 14, 14, 128)  512         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 14, 14, 128)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 14, 14, 32)   36896       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 14, 14, 576)  0           concatenate_26[0][0]             \n",
      "                                                                 conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 14, 14, 576)  2304        concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 14, 14, 576)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 14, 14, 128)  73856       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 14, 14, 128)  512         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 14, 14, 128)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 14, 14, 32)   36896       activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 14, 14, 608)  0           concatenate_27[0][0]             \n",
      "                                                                 conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 14, 14, 608)  2432        concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 14, 14, 608)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 14, 14, 128)  77952       activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 14, 14, 128)  512         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 14, 14, 128)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 14, 14, 32)   36896       activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 14, 14, 640)  0           concatenate_28[0][0]             \n",
      "                                                                 conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 14, 14, 640)  2560        concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 14, 14, 640)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 14, 14, 128)  82048       activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 14, 14, 128)  512         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 14, 14, 128)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 14, 14, 32)   36896       activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 14, 14, 672)  0           concatenate_29[0][0]             \n",
      "                                                                 conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 14, 14, 672)  2688        concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 14, 14, 672)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 14, 14, 128)  86144       activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 14, 14, 128)  512         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 14, 14, 128)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 14, 14, 32)   36896       activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 14, 14, 704)  0           concatenate_30[0][0]             \n",
      "                                                                 conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 14, 14, 704)  2816        concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 14, 14, 704)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 14, 14, 128)  90240       activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 14, 14, 128)  512         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 14, 14, 128)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 14, 14, 32)   36896       activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 14, 14, 736)  0           concatenate_31[0][0]             \n",
      "                                                                 conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 14, 14, 736)  2944        concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 14, 14, 736)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 14, 14, 128)  94336       activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 14, 14, 128)  512         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 14, 14, 128)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 14, 14, 32)   36896       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 14, 14, 768)  0           concatenate_32[0][0]             \n",
      "                                                                 conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 14, 14, 768)  3072        concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 14, 14, 768)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 14, 14, 128)  98432       activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 14, 14, 128)  512         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 14, 14, 128)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 14, 14, 32)   36896       activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 14, 14, 800)  0           concatenate_33[0][0]             \n",
      "                                                                 conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 14, 14, 800)  3200        concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 14, 14, 800)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 14, 14, 128)  102528      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 14, 14, 128)  512         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 14, 14, 128)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 14, 14, 32)   36896       activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 14, 14, 832)  0           concatenate_34[0][0]             \n",
      "                                                                 conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 14, 14, 832)  3328        concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 14, 14, 832)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 14, 14, 128)  106624      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 14, 14, 128)  512         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 14, 14, 128)  0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 14, 14, 32)   36896       activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 14, 14, 864)  0           concatenate_35[0][0]             \n",
      "                                                                 conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 14, 14, 864)  3456        concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 14, 14, 864)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 14, 14, 128)  110720      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 14, 14, 128)  512         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 14, 14, 128)  0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 14, 14, 32)   36896       activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 14, 14, 896)  0           concatenate_36[0][0]             \n",
      "                                                                 conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 14, 14, 896)  3584        concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 14, 14, 896)  0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 14, 14, 128)  114816      activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 14, 14, 128)  512         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 14, 14, 128)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 14, 14, 32)   36896       activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 14, 14, 928)  0           concatenate_37[0][0]             \n",
      "                                                                 conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 14, 14, 928)  3712        concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 14, 14, 928)  0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 14, 14, 128)  118912      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 14, 14, 128)  512         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 14, 14, 128)  0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 14, 14, 32)   36896       activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 14, 14, 960)  0           concatenate_38[0][0]             \n",
      "                                                                 conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 14, 14, 960)  3840        concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 14, 14, 960)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 14, 14, 128)  123008      activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 14, 14, 128)  512         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 14, 14, 128)  0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 14, 14, 32)   36896       activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 14, 14, 992)  0           concatenate_39[0][0]             \n",
      "                                                                 conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 14, 14, 992)  3968        concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 14, 14, 992)  0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 14, 14, 128)  127104      activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 14, 14, 128)  512         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 14, 14, 128)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 14, 14, 32)   36896       activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 14, 14, 1024) 0           concatenate_40[0][0]             \n",
      "                                                                 conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 14, 14, 1024) 4096        concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 14, 14, 1024) 0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 14, 14, 512)  524800      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 7, 7, 512)    0           conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 7, 7, 512)    2048        average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 7, 7, 512)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 7, 7, 128)    65664       activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 7, 7, 128)    512         conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 7, 7, 128)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 7, 7, 32)     36896       activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 7, 7, 544)    0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 7, 7, 544)    2176        concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 7, 7, 544)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 7, 7, 128)    69760       activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 7, 7, 128)    512         conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 7, 7, 128)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 7, 7, 32)     36896       activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 7, 7, 576)    0           concatenate_42[0][0]             \n",
      "                                                                 conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 7, 7, 576)    2304        concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 7, 7, 576)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 7, 7, 128)    73856       activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 7, 7, 128)    512         conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 7, 7, 128)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 7, 7, 32)     36896       activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, 7, 7, 608)    0           concatenate_43[0][0]             \n",
      "                                                                 conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 7, 7, 608)    2432        concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 7, 7, 608)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 7, 7, 128)    77952       activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 7, 7, 128)    512         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 7, 7, 128)    0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 7, 7, 32)     36896       activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, 7, 7, 640)    0           concatenate_44[0][0]             \n",
      "                                                                 conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 7, 7, 640)    2560        concatenate_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 7, 7, 640)    0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 7, 7, 128)    82048       activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 7, 7, 128)    512         conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 7, 7, 128)    0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 7, 7, 32)     36896       activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, 7, 7, 672)    0           concatenate_45[0][0]             \n",
      "                                                                 conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 7, 7, 672)    2688        concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 7, 7, 672)    0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 7, 7, 128)    86144       activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 7, 7, 128)    512         conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 7, 7, 128)    0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 7, 7, 32)     36896       activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_47 (Concatenate)    (None, 7, 7, 704)    0           concatenate_46[0][0]             \n",
      "                                                                 conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 7, 7, 704)    2816        concatenate_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 7, 7, 704)    0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 7, 7, 128)    90240       activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 7, 7, 128)    512         conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 7, 7, 128)    0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 7, 7, 32)     36896       activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_48 (Concatenate)    (None, 7, 7, 736)    0           concatenate_47[0][0]             \n",
      "                                                                 conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 7, 7, 736)    2944        concatenate_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 7, 7, 736)    0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 7, 7, 128)    94336       activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 7, 7, 128)    512         conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 7, 7, 128)    0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 7, 7, 32)     36896       activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_49 (Concatenate)    (None, 7, 7, 768)    0           concatenate_48[0][0]             \n",
      "                                                                 conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 7, 7, 768)    3072        concatenate_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 7, 7, 768)    0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 7, 7, 128)    98432       activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 7, 7, 128)    512         conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 7, 7, 128)    0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 7, 7, 32)     36896       activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_50 (Concatenate)    (None, 7, 7, 800)    0           concatenate_49[0][0]             \n",
      "                                                                 conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 7, 7, 800)    3200        concatenate_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 7, 7, 800)    0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 7, 7, 128)    102528      activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 7, 7, 128)    512         conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 7, 7, 128)    0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 7, 7, 32)     36896       activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_51 (Concatenate)    (None, 7, 7, 832)    0           concatenate_50[0][0]             \n",
      "                                                                 conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 7, 7, 832)    3328        concatenate_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 7, 7, 832)    0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 7, 7, 128)    106624      activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 7, 7, 128)    512         conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 7, 7, 128)    0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 7, 7, 32)     36896       activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_52 (Concatenate)    (None, 7, 7, 864)    0           concatenate_51[0][0]             \n",
      "                                                                 conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 7, 7, 864)    3456        concatenate_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 7, 7, 864)    0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 7, 7, 128)    110720      activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 7, 7, 128)    512         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 7, 7, 128)    0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 7, 7, 32)     36896       activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_53 (Concatenate)    (None, 7, 7, 896)    0           concatenate_52[0][0]             \n",
      "                                                                 conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 7, 7, 896)    3584        concatenate_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 7, 7, 896)    0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 7, 7, 128)    114816      activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 7, 7, 128)    512         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 7, 7, 128)    0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 7, 7, 32)     36896       activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_54 (Concatenate)    (None, 7, 7, 928)    0           concatenate_53[0][0]             \n",
      "                                                                 conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 7, 7, 928)    3712        concatenate_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 7, 7, 928)    0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 7, 7, 128)    118912      activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 7, 7, 128)    512         conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 7, 7, 128)    0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 7, 7, 32)     36896       activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_55 (Concatenate)    (None, 7, 7, 960)    0           concatenate_54[0][0]             \n",
      "                                                                 conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 7, 7, 960)    3840        concatenate_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 7, 7, 960)    0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 7, 7, 128)    123008      activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 7, 7, 128)    512         conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 7, 7, 128)    0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 7, 7, 32)     36896       activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_56 (Concatenate)    (None, 7, 7, 992)    0           concatenate_55[0][0]             \n",
      "                                                                 conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 7, 7, 992)    3968        concatenate_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 7, 7, 992)    0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 7, 7, 128)    127104      activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 7, 7, 128)    512         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 7, 7, 128)    0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 7, 7, 32)     36896       activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_57 (Concatenate)    (None, 7, 7, 1024)   0           concatenate_56[0][0]             \n",
      "                                                                 conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1024)         0           concatenate_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 257)          263425      global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 7,307,073\n",
      "Trainable params: 7,225,473\n",
      "Non-trainable params: 81,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pwRlyWzi1qmm"
   },
   "outputs": [],
   "source": [
    "# 폴더 생성\n",
    "\n",
    "os.makedirs(os.path.join(dir,'model_output',number,model.name), exist_ok=True)\n",
    "os.makedirs(os.path.join(dir,'train_valid_output',number), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X6K-e9yii3Qh"
   },
   "outputs": [],
   "source": [
    "# 참고 : https://github.com/OverLordGoldDragon/keras-adamw\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/Colab Notebooks/Paper')\n",
    "import utils\n",
    "import optimizers_v2\n",
    "from utils import get_weight_decays, fill_dict_in_order\n",
    "from utils import reset_seeds, K_eval\n",
    "from optimizers_v2 import AdamW, NadamW, SGDW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 160578,
     "status": "ok",
     "timestamp": 1599313774933,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "03303793760957673272"
     },
     "user_tz": -540
    },
    "id": "8wgOtwPW1qmp",
    "outputId": "46399b3a-99b1-4dd9-c0b1-b96e44ef07f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cosine annealing learning rates\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model=model, use_cosine_annealing=True, total_iterations = len(x_train) // batch_sizes , eta_min = 1e-2)\n",
    "#optimizer = Adam()\n",
    "filepath =  os.path.join(dir,'model_output',number,model.name,'{epoch:03d}.h5')\n",
    "\n",
    "callbacks_list = [ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_weights_only=False, save_best_only=True, mode='min'),\n",
    "                  #ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_weights_only=False, save_best_only=True, mode='max'),\n",
    "                  #ReduceLROnPlateau(monitor='val_loss',patience=3,factor=0.1,min_lr=1e-5),\n",
    "                  LearningRateScheduler(lr_schedule,verbose=1)\n",
    "                  ]\n",
    "                  \n",
    "model.compile(optimizer, loss = 'categorical_crossentropy', metrics=['accuracy',macro_f1score,weighted_f1score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43538068,
     "status": "ok",
     "timestamp": 1599357152436,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "03303793760957673272"
     },
     "user_tz": -540
    },
    "id": "VmS0d9OS1qmr",
    "outputId": "a122dfdc-eda2-4e44-fd16-2112fecd1d0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 1/70\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_1/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_2/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_3/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_4/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_5/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_6/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_7/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_8/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_9/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_10/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_11/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_12/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_13/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_14/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_15/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_16/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_17/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_18/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_19/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_20/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_21/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_22/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_23/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_24/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_25/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_26/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_27/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_28/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_29/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_30/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_31/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_32/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_33/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_34/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_35/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_36/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_37/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_38/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_39/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_40/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_41/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_42/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_43/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_44/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_45/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_46/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_47/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_48/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_49/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_50/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_51/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_52/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_53/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_54/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_55/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_56/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_57/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_58/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_59/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_60/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_61/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_62/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_63/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_64/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_65/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_66/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_67/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_68/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_69/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_70/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_71/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_72/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_73/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_74/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_75/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_76/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_77/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_78/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_79/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_80/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_81/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_82/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_83/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_84/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_85/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_86/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_87/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_88/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_89/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_90/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_91/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_92/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_93/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_94/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_95/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_96/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_97/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_98/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_99/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_100/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_101/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_102/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_103/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_104/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_105/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_106/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_107/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_108/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_109/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_110/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_111/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_112/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_113/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_114/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_115/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_116/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_117/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_118/kernel:0\n",
      "0.0(L1), 2.0465779886856937e-06(L2) weight decay set for conv2d_119/kernel:0\n",
      "382/382 [==============================] - ETA: 0s - loss: 4.9676 - accuracy: 0.0992 - macro_f1score: 0.0022 - weighted_f1score: 5.9496e-05 \n",
      "Epoch 00001: val_loss improved from inf to 4.57375, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/001.h5\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.12636, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/001.h5\n",
      "382/382 [==============================] - 13531s 35s/step - loss: 4.9676 - accuracy: 0.0992 - macro_f1score: 0.0022 - weighted_f1score: 5.9496e-05 - val_loss: 4.5737 - val_accuracy: 0.1264 - val_macro_f1score: 0.0021 - val_weighted_f1score: 5.9444e-05\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 2/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 4.4942 - accuracy: 0.1406 - macro_f1score: 0.0054 - weighted_f1score: 1.4498e-04\n",
      "Epoch 00002: val_loss improved from 4.57375 to 4.24004, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/002.h5\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.12636 to 0.17357, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/002.h5\n",
      "382/382 [==============================] - 432s 1s/step - loss: 4.4942 - accuracy: 0.1406 - macro_f1score: 0.0054 - weighted_f1score: 1.4498e-04 - val_loss: 4.2400 - val_accuracy: 0.1736 - val_macro_f1score: 0.0068 - val_weighted_f1score: 2.0179e-04\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 3/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 4.1887 - accuracy: 0.1705 - macro_f1score: 0.0088 - weighted_f1score: 2.3243e-04\n",
      "Epoch 00003: val_loss improved from 4.24004 to 3.98060, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/003.h5\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.17357 to 0.20992, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/003.h5\n",
      "382/382 [==============================] - 422s 1s/step - loss: 4.1887 - accuracy: 0.1705 - macro_f1score: 0.0088 - weighted_f1score: 2.3243e-04 - val_loss: 3.9806 - val_accuracy: 0.2099 - val_macro_f1score: 0.0105 - val_weighted_f1score: 2.7639e-04\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 4/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 3.9254 - accuracy: 0.2062 - macro_f1score: 0.0117 - weighted_f1score: 2.9961e-04\n",
      "Epoch 00004: val_loss improved from 3.98060 to 3.74823, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/004.h5\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.20992 to 0.24830, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/004.h5\n",
      "382/382 [==============================] - 417s 1s/step - loss: 3.9254 - accuracy: 0.2062 - macro_f1score: 0.0117 - weighted_f1score: 2.9961e-04 - val_loss: 3.7482 - val_accuracy: 0.2483 - val_macro_f1score: 0.0149 - val_weighted_f1score: 3.7269e-04\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 5/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 3.6815 - accuracy: 0.2399 - macro_f1score: 0.0153 - weighted_f1score: 3.7695e-04\n",
      "Epoch 00005: val_loss improved from 3.74823 to 3.52350, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/005.h5\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.24830 to 0.27582, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/005.h5\n",
      "382/382 [==============================] - 420s 1s/step - loss: 3.6815 - accuracy: 0.2399 - macro_f1score: 0.0153 - weighted_f1score: 3.7695e-04 - val_loss: 3.5235 - val_accuracy: 0.2758 - val_macro_f1score: 0.0202 - val_weighted_f1score: 4.7201e-04\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 6/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 3.4539 - accuracy: 0.2684 - macro_f1score: 0.0197 - weighted_f1score: 4.5704e-04\n",
      "Epoch 00006: val_loss improved from 3.52350 to 3.31241, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/006.h5\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.27582 to 0.31046, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/006.h5\n",
      "382/382 [==============================] - 421s 1s/step - loss: 3.4539 - accuracy: 0.2684 - macro_f1score: 0.0197 - weighted_f1score: 4.5704e-04 - val_loss: 3.3124 - val_accuracy: 0.3105 - val_macro_f1score: 0.0245 - val_weighted_f1score: 5.5016e-04\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 7/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 3.2411 - accuracy: 0.3005 - macro_f1score: 0.0242 - weighted_f1score: 5.5002e-04\n",
      "Epoch 00007: val_loss improved from 3.31241 to 3.10985, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/007.h5\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.31046 to 0.34103, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/007.h5\n",
      "382/382 [==============================] - 423s 1s/step - loss: 3.2411 - accuracy: 0.3005 - macro_f1score: 0.0242 - weighted_f1score: 5.5002e-04 - val_loss: 3.1098 - val_accuracy: 0.3410 - val_macro_f1score: 0.0295 - val_weighted_f1score: 6.7397e-04\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 8/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 3.0457 - accuracy: 0.3333 - macro_f1score: 0.0302 - weighted_f1score: 6.5810e-04\n",
      "Epoch 00008: val_loss improved from 3.10985 to 2.95025, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/008.h5\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.34103 to 0.36311, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/008.h5\n",
      "382/382 [==============================] - 419s 1s/step - loss: 3.0457 - accuracy: 0.3333 - macro_f1score: 0.0302 - weighted_f1score: 6.5810e-04 - val_loss: 2.9503 - val_accuracy: 0.3631 - val_macro_f1score: 0.0370 - val_weighted_f1score: 7.9640e-04\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 9/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 2.8942 - accuracy: 0.3588 - macro_f1score: 0.0352 - weighted_f1score: 7.5126e-04\n",
      "Epoch 00009: val_loss improved from 2.95025 to 2.85882, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/009.h5\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.36311 to 0.38587, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/009.h5\n",
      "382/382 [==============================] - 418s 1s/step - loss: 2.8942 - accuracy: 0.3588 - macro_f1score: 0.0352 - weighted_f1score: 7.5126e-04 - val_loss: 2.8588 - val_accuracy: 0.3859 - val_macro_f1score: 0.0420 - val_weighted_f1score: 8.9584e-04\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 10/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 2.7312 - accuracy: 0.3907 - macro_f1score: 0.0409 - weighted_f1score: 8.5929e-04\n",
      "Epoch 00010: val_loss improved from 2.85882 to 2.72760, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/010.h5\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.38587 to 0.41780, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/010.h5\n",
      "382/382 [==============================] - 423s 1s/step - loss: 2.7312 - accuracy: 0.3907 - macro_f1score: 0.0409 - weighted_f1score: 8.5929e-04 - val_loss: 2.7276 - val_accuracy: 0.4178 - val_macro_f1score: 0.0474 - val_weighted_f1score: 9.7628e-04\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 11/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 2.5802 - accuracy: 0.4129 - macro_f1score: 0.0470 - weighted_f1score: 9.7045e-04\n",
      "Epoch 00011: val_loss improved from 2.72760 to 2.62656, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/011.h5\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.41780 to 0.42765, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/011.h5\n",
      "382/382 [==============================] - 424s 1s/step - loss: 2.5802 - accuracy: 0.4129 - macro_f1score: 0.0470 - weighted_f1score: 9.7045e-04 - val_loss: 2.6266 - val_accuracy: 0.4276 - val_macro_f1score: 0.0540 - val_weighted_f1score: 0.0011\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 12/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 2.4663 - accuracy: 0.4344 - macro_f1score: 0.0509 - weighted_f1score: 0.0011\n",
      "Epoch 00012: val_loss improved from 2.62656 to 2.58493, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/012.h5\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.42765 to 0.44395, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/012.h5\n",
      "382/382 [==============================] - 411s 1s/step - loss: 2.4663 - accuracy: 0.4344 - macro_f1score: 0.0509 - weighted_f1score: 0.0011 - val_loss: 2.5849 - val_accuracy: 0.4440 - val_macro_f1score: 0.0582 - val_weighted_f1score: 0.0012\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 13/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 2.3433 - accuracy: 0.4568 - macro_f1score: 0.0570 - weighted_f1score: 0.0012\n",
      "Epoch 00013: val_loss improved from 2.58493 to 2.49734, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/013.h5\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.44395 to 0.46399, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/013.h5\n",
      "382/382 [==============================] - 407s 1s/step - loss: 2.3433 - accuracy: 0.4568 - macro_f1score: 0.0570 - weighted_f1score: 0.0012 - val_loss: 2.4973 - val_accuracy: 0.4640 - val_macro_f1score: 0.0631 - val_weighted_f1score: 0.0013\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 14/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 2.2207 - accuracy: 0.4816 - macro_f1score: 0.0633 - weighted_f1score: 0.0013\n",
      "Epoch 00014: val_loss improved from 2.49734 to 2.42253, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/014.h5\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.46399 to 0.47656, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/014.h5\n",
      "382/382 [==============================] - 406s 1s/step - loss: 2.2207 - accuracy: 0.4816 - macro_f1score: 0.0633 - weighted_f1score: 0.0013 - val_loss: 2.4225 - val_accuracy: 0.4766 - val_macro_f1score: 0.0676 - val_weighted_f1score: 0.0013\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 15/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 2.1658 - accuracy: 0.4904 - macro_f1score: 0.0666 - weighted_f1score: 0.0013\n",
      "Epoch 00015: val_loss improved from 2.42253 to 2.40378, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/015.h5\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.47656 to 0.47826, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/015.h5\n",
      "382/382 [==============================] - 404s 1s/step - loss: 2.1658 - accuracy: 0.4904 - macro_f1score: 0.0666 - weighted_f1score: 0.0013 - val_loss: 2.4038 - val_accuracy: 0.4783 - val_macro_f1score: 0.0723 - val_weighted_f1score: 0.0014\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 16/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 2.0292 - accuracy: 0.5186 - macro_f1score: 0.0733 - weighted_f1score: 0.0014\n",
      "Epoch 00016: val_loss improved from 2.40378 to 2.37759, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/016.h5\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.47826 to 0.48607, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/016.h5\n",
      "382/382 [==============================] - 405s 1s/step - loss: 2.0292 - accuracy: 0.5186 - macro_f1score: 0.0733 - weighted_f1score: 0.0014 - val_loss: 2.3776 - val_accuracy: 0.4861 - val_macro_f1score: 0.0742 - val_weighted_f1score: 0.0015\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 17/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 1.9516 - accuracy: 0.5332 - macro_f1score: 0.0771 - weighted_f1score: 0.0015\n",
      "Epoch 00017: val_loss improved from 2.37759 to 2.34644, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/017.h5\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.48607 to 0.50068, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/017.h5\n",
      "382/382 [==============================] - 414s 1s/step - loss: 1.9516 - accuracy: 0.5332 - macro_f1score: 0.0771 - weighted_f1score: 0.0015 - val_loss: 2.3464 - val_accuracy: 0.5007 - val_macro_f1score: 0.0777 - val_weighted_f1score: 0.0015\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 18/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 1.8623 - accuracy: 0.5495 - macro_f1score: 0.0818 - weighted_f1score: 0.0016\n",
      "Epoch 00018: val_loss improved from 2.34644 to 2.28947, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/018.h5\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.50068\n",
      "382/382 [==============================] - 412s 1s/step - loss: 1.8623 - accuracy: 0.5495 - macro_f1score: 0.0818 - weighted_f1score: 0.0016 - val_loss: 2.2895 - val_accuracy: 0.4980 - val_macro_f1score: 0.0773 - val_weighted_f1score: 0.0015\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 19/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 1.7732 - accuracy: 0.5654 - macro_f1score: 0.0871 - weighted_f1score: 0.0017\n",
      "Epoch 00019: val_loss improved from 2.28947 to 2.25005, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/019.h5\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.50068 to 0.50611, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/019.h5\n",
      "382/382 [==============================] - 416s 1s/step - loss: 1.7732 - accuracy: 0.5654 - macro_f1score: 0.0871 - weighted_f1score: 0.0017 - val_loss: 2.2500 - val_accuracy: 0.5061 - val_macro_f1score: 0.0824 - val_weighted_f1score: 0.0016\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 20/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 1.6600 - accuracy: 0.5866 - macro_f1score: 0.0923 - weighted_f1score: 0.0018\n",
      "Epoch 00020: val_loss improved from 2.25005 to 2.23095, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/020.h5\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.50611 to 0.51800, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/020.h5\n",
      "382/382 [==============================] - 422s 1s/step - loss: 1.6600 - accuracy: 0.5866 - macro_f1score: 0.0923 - weighted_f1score: 0.0018 - val_loss: 2.2309 - val_accuracy: 0.5180 - val_macro_f1score: 0.0841 - val_weighted_f1score: 0.0017\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 21/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 1.5800 - accuracy: 0.6081 - macro_f1score: 0.0974 - weighted_f1score: 0.0019\n",
      "Epoch 00021: val_loss improved from 2.23095 to 2.20803, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/021.h5\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.51800 to 0.52751, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/021.h5\n",
      "382/382 [==============================] - 422s 1s/step - loss: 1.5800 - accuracy: 0.6081 - macro_f1score: 0.0974 - weighted_f1score: 0.0019 - val_loss: 2.2080 - val_accuracy: 0.5275 - val_macro_f1score: 0.0877 - val_weighted_f1score: 0.0017\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 22/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 1.5045 - accuracy: 0.6193 - macro_f1score: 0.1019 - weighted_f1score: 0.0020\n",
      "Epoch 00022: val_loss did not improve from 2.20803\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.52751\n",
      "382/382 [==============================] - 412s 1s/step - loss: 1.5045 - accuracy: 0.6193 - macro_f1score: 0.1019 - weighted_f1score: 0.0020 - val_loss: 2.2152 - val_accuracy: 0.5245 - val_macro_f1score: 0.0863 - val_weighted_f1score: 0.0017\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 23/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 1.4650 - accuracy: 0.6292 - macro_f1score: 0.1045 - weighted_f1score: 0.0020\n",
      "Epoch 00023: val_loss did not improve from 2.20803\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.52751 to 0.53023, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/023.h5\n",
      "382/382 [==============================] - 415s 1s/step - loss: 1.4650 - accuracy: 0.6292 - macro_f1score: 0.1045 - weighted_f1score: 0.0020 - val_loss: 2.2125 - val_accuracy: 0.5302 - val_macro_f1score: 0.0905 - val_weighted_f1score: 0.0018\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 24/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 1.3649 - accuracy: 0.6499 - macro_f1score: 0.1110 - weighted_f1score: 0.0021\n",
      "Epoch 00024: val_loss did not improve from 2.20803\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.53023\n",
      "382/382 [==============================] - 410s 1s/step - loss: 1.3649 - accuracy: 0.6499 - macro_f1score: 0.1110 - weighted_f1score: 0.0021 - val_loss: 2.2143 - val_accuracy: 0.5302 - val_macro_f1score: 0.0941 - val_weighted_f1score: 0.0018\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 25/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 1.3010 - accuracy: 0.6632 - macro_f1score: 0.1139 - weighted_f1score: 0.0022\n",
      "Epoch 00025: val_loss improved from 2.20803 to 2.20678, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/025.h5\n",
      "\n",
      "Epoch 00025: val_accuracy improved from 0.53023 to 0.53533, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/025.h5\n",
      "382/382 [==============================] - 402s 1s/step - loss: 1.3010 - accuracy: 0.6632 - macro_f1score: 0.1139 - weighted_f1score: 0.0022 - val_loss: 2.2068 - val_accuracy: 0.5353 - val_macro_f1score: 0.0946 - val_weighted_f1score: 0.0018\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 26/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 1.2863 - accuracy: 0.6681 - macro_f1score: 0.1156 - weighted_f1score: 0.0022\n",
      "Epoch 00026: val_loss improved from 2.20678 to 2.18541, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/026.h5\n",
      "\n",
      "Epoch 00026: val_accuracy improved from 0.53533 to 0.54959, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/026.h5\n",
      "382/382 [==============================] - 405s 1s/step - loss: 1.2863 - accuracy: 0.6681 - macro_f1score: 0.1156 - weighted_f1score: 0.0022 - val_loss: 2.1854 - val_accuracy: 0.5496 - val_macro_f1score: 0.0975 - val_weighted_f1score: 0.0019\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 27/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 1.2272 - accuracy: 0.6780 - macro_f1score: 0.1199 - weighted_f1score: 0.0023\n",
      "Epoch 00027: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00027: val_accuracy improved from 0.54959 to 0.55231, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/027.h5\n",
      "382/382 [==============================] - 401s 1s/step - loss: 1.2272 - accuracy: 0.6780 - macro_f1score: 0.1199 - weighted_f1score: 0.0023 - val_loss: 2.2178 - val_accuracy: 0.5523 - val_macro_f1score: 0.0955 - val_weighted_f1score: 0.0018\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 28/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 1.1435 - accuracy: 0.7013 - macro_f1score: 0.1252 - weighted_f1score: 0.0024\n",
      "Epoch 00028: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.55231\n",
      "382/382 [==============================] - 397s 1s/step - loss: 1.1435 - accuracy: 0.7013 - macro_f1score: 0.1252 - weighted_f1score: 0.0024 - val_loss: 2.2204 - val_accuracy: 0.5414 - val_macro_f1score: 0.0969 - val_weighted_f1score: 0.0019\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 29/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 1.1182 - accuracy: 0.7048 - macro_f1score: 0.1274 - weighted_f1score: 0.0024\n",
      "Epoch 00029: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.55231\n",
      "382/382 [==============================] - 411s 1s/step - loss: 1.1182 - accuracy: 0.7048 - macro_f1score: 0.1274 - weighted_f1score: 0.0024 - val_loss: 2.2402 - val_accuracy: 0.5469 - val_macro_f1score: 0.0979 - val_weighted_f1score: 0.0019\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 30/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 1.0136 - accuracy: 0.7262 - macro_f1score: 0.1325 - weighted_f1score: 0.0025\n",
      "Epoch 00030: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.55231\n",
      "382/382 [==============================] - 421s 1s/step - loss: 1.0136 - accuracy: 0.7262 - macro_f1score: 0.1325 - weighted_f1score: 0.0025 - val_loss: 2.2585 - val_accuracy: 0.5479 - val_macro_f1score: 0.0978 - val_weighted_f1score: 0.0019\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 31/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.9503 - accuracy: 0.7445 - macro_f1score: 0.1380 - weighted_f1score: 0.0026\n",
      "Epoch 00031: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00031: val_accuracy improved from 0.55231 to 0.55299, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/031.h5\n",
      "382/382 [==============================] - 425s 1s/step - loss: 0.9503 - accuracy: 0.7445 - macro_f1score: 0.1380 - weighted_f1score: 0.0026 - val_loss: 2.2495 - val_accuracy: 0.5530 - val_macro_f1score: 0.1023 - val_weighted_f1score: 0.0020\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 32/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.8980 - accuracy: 0.7543 - macro_f1score: 0.1410 - weighted_f1score: 0.0027\n",
      "Epoch 00032: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.55299\n",
      "382/382 [==============================] - 415s 1s/step - loss: 0.8980 - accuracy: 0.7543 - macro_f1score: 0.1410 - weighted_f1score: 0.0027 - val_loss: 2.2791 - val_accuracy: 0.5503 - val_macro_f1score: 0.1008 - val_weighted_f1score: 0.0019\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 33/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.8530 - accuracy: 0.7631 - macro_f1score: 0.1438 - weighted_f1score: 0.0027\n",
      "Epoch 00033: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00033: val_accuracy improved from 0.55299 to 0.55435, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/033.h5\n",
      "382/382 [==============================] - 425s 1s/step - loss: 0.8530 - accuracy: 0.7631 - macro_f1score: 0.1438 - weighted_f1score: 0.0027 - val_loss: 2.2858 - val_accuracy: 0.5543 - val_macro_f1score: 0.1009 - val_weighted_f1score: 0.0020\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 34/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.8021 - accuracy: 0.7784 - macro_f1score: 0.1484 - weighted_f1score: 0.0028\n",
      "Epoch 00034: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.55435\n",
      "382/382 [==============================] - 424s 1s/step - loss: 0.8021 - accuracy: 0.7784 - macro_f1score: 0.1484 - weighted_f1score: 0.0028 - val_loss: 2.2916 - val_accuracy: 0.5506 - val_macro_f1score: 0.1025 - val_weighted_f1score: 0.0020\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 35/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.7690 - accuracy: 0.7830 - macro_f1score: 0.1506 - weighted_f1score: 0.0028\n",
      "Epoch 00035: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.55435\n",
      "382/382 [==============================] - 430s 1s/step - loss: 0.7690 - accuracy: 0.7830 - macro_f1score: 0.1506 - weighted_f1score: 0.0028 - val_loss: 2.3563 - val_accuracy: 0.5540 - val_macro_f1score: 0.1027 - val_weighted_f1score: 0.0020\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 36/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.7086 - accuracy: 0.7985 - macro_f1score: 0.1547 - weighted_f1score: 0.0029\n",
      "Epoch 00036: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.55435\n",
      "382/382 [==============================] - 433s 1s/step - loss: 0.7086 - accuracy: 0.7985 - macro_f1score: 0.1547 - weighted_f1score: 0.0029 - val_loss: 2.2933 - val_accuracy: 0.5543 - val_macro_f1score: 0.1035 - val_weighted_f1score: 0.0020\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 37/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.6920 - accuracy: 0.8018 - macro_f1score: 0.1559 - weighted_f1score: 0.0029\n",
      "Epoch 00037: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00037: val_accuracy improved from 0.55435 to 0.56148, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/037.h5\n",
      "382/382 [==============================] - 440s 1s/step - loss: 0.6920 - accuracy: 0.8018 - macro_f1score: 0.1559 - weighted_f1score: 0.0029 - val_loss: 2.3560 - val_accuracy: 0.5615 - val_macro_f1score: 0.1034 - val_weighted_f1score: 0.0020\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 38/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.6432 - accuracy: 0.8161 - macro_f1score: 0.1597 - weighted_f1score: 0.0030\n",
      "Epoch 00038: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.56148\n",
      "382/382 [==============================] - 442s 1s/step - loss: 0.6432 - accuracy: 0.8161 - macro_f1score: 0.1597 - weighted_f1score: 0.0030 - val_loss: 2.4652 - val_accuracy: 0.5489 - val_macro_f1score: 0.1045 - val_weighted_f1score: 0.0020\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 39/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.6697 - accuracy: 0.8092 - macro_f1score: 0.1589 - weighted_f1score: 0.0030\n",
      "Epoch 00039: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.56148\n",
      "382/382 [==============================] - 444s 1s/step - loss: 0.6697 - accuracy: 0.8092 - macro_f1score: 0.1589 - weighted_f1score: 0.0030 - val_loss: 2.4724 - val_accuracy: 0.5577 - val_macro_f1score: 0.1044 - val_weighted_f1score: 0.0020\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 40/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.6065 - accuracy: 0.8244 - macro_f1score: 0.1632 - weighted_f1score: 0.0030\n",
      "Epoch 00040: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.56148\n",
      "382/382 [==============================] - 444s 1s/step - loss: 0.6065 - accuracy: 0.8244 - macro_f1score: 0.1632 - weighted_f1score: 0.0030 - val_loss: 2.4886 - val_accuracy: 0.5479 - val_macro_f1score: 0.1039 - val_weighted_f1score: 0.0020\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 41/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.5805 - accuracy: 0.8321 - macro_f1score: 0.1651 - weighted_f1score: 0.0031\n",
      "Epoch 00041: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.56148\n",
      "382/382 [==============================] - 452s 1s/step - loss: 0.5805 - accuracy: 0.8321 - macro_f1score: 0.1651 - weighted_f1score: 0.0031 - val_loss: 2.5169 - val_accuracy: 0.5598 - val_macro_f1score: 0.1050 - val_weighted_f1score: 0.0021\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 42/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.5205 - accuracy: 0.8476 - macro_f1score: 0.1686 - weighted_f1score: 0.0031\n",
      "Epoch 00042: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00042: val_accuracy improved from 0.56148 to 0.56216, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/042.h5\n",
      "382/382 [==============================] - 454s 1s/step - loss: 0.5205 - accuracy: 0.8476 - macro_f1score: 0.1686 - weighted_f1score: 0.0031 - val_loss: 2.5579 - val_accuracy: 0.5622 - val_macro_f1score: 0.1062 - val_weighted_f1score: 0.0021\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 43/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.5176 - accuracy: 0.8492 - macro_f1score: 0.1696 - weighted_f1score: 0.0032\n",
      "Epoch 00043: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.56216\n",
      "382/382 [==============================] - 456s 1s/step - loss: 0.5176 - accuracy: 0.8492 - macro_f1score: 0.1696 - weighted_f1score: 0.0032 - val_loss: 2.5133 - val_accuracy: 0.5557 - val_macro_f1score: 0.1062 - val_weighted_f1score: 0.0020\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 44/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.4807 - accuracy: 0.8556 - macro_f1score: 0.1721 - weighted_f1score: 0.0032\n",
      "Epoch 00044: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.56216\n",
      "382/382 [==============================] - 454s 1s/step - loss: 0.4807 - accuracy: 0.8556 - macro_f1score: 0.1721 - weighted_f1score: 0.0032 - val_loss: 2.5885 - val_accuracy: 0.5506 - val_macro_f1score: 0.1058 - val_weighted_f1score: 0.0020\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 45/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.4443 - accuracy: 0.8695 - macro_f1score: 0.1759 - weighted_f1score: 0.0033\n",
      "Epoch 00045: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.56216\n",
      "382/382 [==============================] - 456s 1s/step - loss: 0.4443 - accuracy: 0.8695 - macro_f1score: 0.1759 - weighted_f1score: 0.0033 - val_loss: 2.6113 - val_accuracy: 0.5486 - val_macro_f1score: 0.1072 - val_weighted_f1score: 0.0020\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 46/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.4503 - accuracy: 0.8678 - macro_f1score: 0.1755 - weighted_f1score: 0.0033\n",
      "Epoch 00046: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.56216\n",
      "382/382 [==============================] - 454s 1s/step - loss: 0.4503 - accuracy: 0.8678 - macro_f1score: 0.1755 - weighted_f1score: 0.0033 - val_loss: 2.6205 - val_accuracy: 0.5577 - val_macro_f1score: 0.1081 - val_weighted_f1score: 0.0021\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 47/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.4158 - accuracy: 0.8759 - macro_f1score: 0.1778 - weighted_f1score: 0.0033\n",
      "Epoch 00047: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.56216\n",
      "382/382 [==============================] - 453s 1s/step - loss: 0.4158 - accuracy: 0.8759 - macro_f1score: 0.1778 - weighted_f1score: 0.0033 - val_loss: 2.6701 - val_accuracy: 0.5571 - val_macro_f1score: 0.1084 - val_weighted_f1score: 0.0021\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 48/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.3907 - accuracy: 0.8861 - macro_f1score: 0.1803 - weighted_f1score: 0.0033\n",
      "Epoch 00048: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.56216\n",
      "382/382 [==============================] - 446s 1s/step - loss: 0.3907 - accuracy: 0.8861 - macro_f1score: 0.1803 - weighted_f1score: 0.0033 - val_loss: 2.6967 - val_accuracy: 0.5550 - val_macro_f1score: 0.1065 - val_weighted_f1score: 0.0021\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 49/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.3718 - accuracy: 0.8874 - macro_f1score: 0.1810 - weighted_f1score: 0.0033\n",
      "Epoch 00049: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.56216\n",
      "382/382 [==============================] - 446s 1s/step - loss: 0.3718 - accuracy: 0.8874 - macro_f1score: 0.1810 - weighted_f1score: 0.0033 - val_loss: 2.7071 - val_accuracy: 0.5594 - val_macro_f1score: 0.1087 - val_weighted_f1score: 0.0021\n",
      "Learning rate:  0.001\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 50/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.3977 - accuracy: 0.8815 - macro_f1score: 0.1801 - weighted_f1score: 0.0033\n",
      "Epoch 00050: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.56216\n",
      "382/382 [==============================] - 451s 1s/step - loss: 0.3977 - accuracy: 0.8815 - macro_f1score: 0.1801 - weighted_f1score: 0.0033 - val_loss: 2.7527 - val_accuracy: 0.5608 - val_macro_f1score: 0.1105 - val_weighted_f1score: 0.0021\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 51/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.1223 - accuracy: 0.9657 - macro_f1score: 0.2019 - weighted_f1score: 0.0037\n",
      "Epoch 00051: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00051: val_accuracy improved from 0.56216 to 0.56929, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/051.h5\n",
      "382/382 [==============================] - 457s 1s/step - loss: 0.1223 - accuracy: 0.9657 - macro_f1score: 0.2019 - weighted_f1score: 0.0037 - val_loss: 2.7456 - val_accuracy: 0.5693 - val_macro_f1score: 0.1125 - val_weighted_f1score: 0.0021\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 52/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.0983 - accuracy: 0.9728 - macro_f1score: 0.2044 - weighted_f1score: 0.0038\n",
      "Epoch 00052: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.56929\n",
      "382/382 [==============================] - 452s 1s/step - loss: 0.0983 - accuracy: 0.9728 - macro_f1score: 0.2044 - weighted_f1score: 0.0038 - val_loss: 2.7798 - val_accuracy: 0.5673 - val_macro_f1score: 0.1105 - val_weighted_f1score: 0.0021\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 53/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.0865 - accuracy: 0.9757 - macro_f1score: 0.2052 - weighted_f1score: 0.0038\n",
      "Epoch 00053: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00053: val_accuracy improved from 0.56929 to 0.57235, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/053.h5\n",
      "382/382 [==============================] - 443s 1s/step - loss: 0.0865 - accuracy: 0.9757 - macro_f1score: 0.2052 - weighted_f1score: 0.0038 - val_loss: 2.8138 - val_accuracy: 0.5724 - val_macro_f1score: 0.1110 - val_weighted_f1score: 0.0021\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 54/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9793 - macro_f1score: 0.2062 - weighted_f1score: 0.0038\n",
      "Epoch 00054: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00054: val_accuracy improved from 0.57235 to 0.57269, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/054.h5\n",
      "382/382 [==============================] - 433s 1s/step - loss: 0.0769 - accuracy: 0.9793 - macro_f1score: 0.2062 - weighted_f1score: 0.0038 - val_loss: 2.8535 - val_accuracy: 0.5727 - val_macro_f1score: 0.1137 - val_weighted_f1score: 0.0022\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 55/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9807 - macro_f1score: 0.2064 - weighted_f1score: 0.0038\n",
      "Epoch 00055: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00055: val_accuracy improved from 0.57269 to 0.57405, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/055.h5\n",
      "382/382 [==============================] - 449s 1s/step - loss: 0.0722 - accuracy: 0.9807 - macro_f1score: 0.2064 - weighted_f1score: 0.0038 - val_loss: 2.8198 - val_accuracy: 0.5740 - val_macro_f1score: 0.1125 - val_weighted_f1score: 0.0022\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 56/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9838 - macro_f1score: 0.2079 - weighted_f1score: 0.0038\n",
      "Epoch 00056: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.57405\n",
      "382/382 [==============================] - 443s 1s/step - loss: 0.0630 - accuracy: 0.9838 - macro_f1score: 0.2079 - weighted_f1score: 0.0038 - val_loss: 2.9000 - val_accuracy: 0.5693 - val_macro_f1score: 0.1128 - val_weighted_f1score: 0.0022\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 57/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.0601 - accuracy: 0.9839 - macro_f1score: 0.2080 - weighted_f1score: 0.0038\n",
      "Epoch 00057: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.57405\n",
      "382/382 [==============================] - 435s 1s/step - loss: 0.0601 - accuracy: 0.9839 - macro_f1score: 0.2080 - weighted_f1score: 0.0038 - val_loss: 2.9490 - val_accuracy: 0.5686 - val_macro_f1score: 0.1127 - val_weighted_f1score: 0.0022\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 58/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 0.9866 - macro_f1score: 0.2082 - weighted_f1score: 0.0038\n",
      "Epoch 00058: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.57405\n",
      "382/382 [==============================] - 440s 1s/step - loss: 0.0524 - accuracy: 0.9866 - macro_f1score: 0.2082 - weighted_f1score: 0.0038 - val_loss: 2.9683 - val_accuracy: 0.5656 - val_macro_f1score: 0.1134 - val_weighted_f1score: 0.0021\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 59/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 0.9860 - macro_f1score: 0.2089 - weighted_f1score: 0.0038\n",
      "Epoch 00059: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.57405\n",
      "382/382 [==============================] - 443s 1s/step - loss: 0.0532 - accuracy: 0.9860 - macro_f1score: 0.2089 - weighted_f1score: 0.0038 - val_loss: 2.9524 - val_accuracy: 0.5696 - val_macro_f1score: 0.1143 - val_weighted_f1score: 0.0022\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 60/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9863 - macro_f1score: 0.2088 - weighted_f1score: 0.0038\n",
      "Epoch 00060: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.57405\n",
      "382/382 [==============================] - 441s 1s/step - loss: 0.0492 - accuracy: 0.9863 - macro_f1score: 0.2088 - weighted_f1score: 0.0038 - val_loss: 3.0107 - val_accuracy: 0.5703 - val_macro_f1score: 0.1124 - val_weighted_f1score: 0.0022\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 61/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9877 - macro_f1score: 0.2087 - weighted_f1score: 0.0038\n",
      "Epoch 00061: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.57405\n",
      "382/382 [==============================] - 441s 1s/step - loss: 0.0478 - accuracy: 0.9877 - macro_f1score: 0.2087 - weighted_f1score: 0.0038 - val_loss: 3.0265 - val_accuracy: 0.5686 - val_macro_f1score: 0.1128 - val_weighted_f1score: 0.0022\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 62/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.0419 - accuracy: 0.9895 - macro_f1score: 0.2097 - weighted_f1score: 0.0038\n",
      "Epoch 00062: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.57405\n",
      "382/382 [==============================] - 442s 1s/step - loss: 0.0419 - accuracy: 0.9895 - macro_f1score: 0.2097 - weighted_f1score: 0.0038 - val_loss: 3.0434 - val_accuracy: 0.5703 - val_macro_f1score: 0.1129 - val_weighted_f1score: 0.0022\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 63/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9894 - macro_f1score: 0.2092 - weighted_f1score: 0.0038\n",
      "Epoch 00063: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.57405\n",
      "382/382 [==============================] - 436s 1s/step - loss: 0.0405 - accuracy: 0.9894 - macro_f1score: 0.2092 - weighted_f1score: 0.0038 - val_loss: 3.0306 - val_accuracy: 0.5730 - val_macro_f1score: 0.1144 - val_weighted_f1score: 0.0022\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 64/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.0416 - accuracy: 0.9890 - macro_f1score: 0.2082 - weighted_f1score: 0.0038\n",
      "Epoch 00064: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.57405\n",
      "382/382 [==============================] - 418s 1s/step - loss: 0.0416 - accuracy: 0.9890 - macro_f1score: 0.2082 - weighted_f1score: 0.0038 - val_loss: 3.0594 - val_accuracy: 0.5693 - val_macro_f1score: 0.1141 - val_weighted_f1score: 0.0022\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 65/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9918 - macro_f1score: 0.2088 - weighted_f1score: 0.0039\n",
      "Epoch 00065: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.57405\n",
      "382/382 [==============================] - 438s 1s/step - loss: 0.0350 - accuracy: 0.9918 - macro_f1score: 0.2088 - weighted_f1score: 0.0039 - val_loss: 3.0741 - val_accuracy: 0.5669 - val_macro_f1score: 0.1132 - val_weighted_f1score: 0.0022\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 66/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9899 - macro_f1score: 0.2093 - weighted_f1score: 0.0038\n",
      "Epoch 00066: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.57405\n",
      "382/382 [==============================] - 443s 1s/step - loss: 0.0373 - accuracy: 0.9899 - macro_f1score: 0.2093 - weighted_f1score: 0.0038 - val_loss: 3.1665 - val_accuracy: 0.5669 - val_macro_f1score: 0.1130 - val_weighted_f1score: 0.0022\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 67/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9900 - macro_f1score: 0.2088 - weighted_f1score: 0.0038\n",
      "Epoch 00067: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.57405\n",
      "382/382 [==============================] - 438s 1s/step - loss: 0.0362 - accuracy: 0.9900 - macro_f1score: 0.2088 - weighted_f1score: 0.0038 - val_loss: 3.1415 - val_accuracy: 0.5717 - val_macro_f1score: 0.1127 - val_weighted_f1score: 0.0022\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 68/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9915 - macro_f1score: 0.2106 - weighted_f1score: 0.0039\n",
      "Epoch 00068: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.57405\n",
      "382/382 [==============================] - 436s 1s/step - loss: 0.0327 - accuracy: 0.9915 - macro_f1score: 0.2106 - weighted_f1score: 0.0039 - val_loss: 3.1747 - val_accuracy: 0.5696 - val_macro_f1score: 0.1136 - val_weighted_f1score: 0.0022\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 69/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9908 - macro_f1score: 0.2105 - weighted_f1score: 0.0039\n",
      "Epoch 00069: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00069: val_accuracy improved from 0.57405 to 0.57439, saving model to /content/drive/My Drive/Colab Notebooks/Paper/CALTECH/No_GAN/model_output/1/DenseNet121/069.h5\n",
      "382/382 [==============================] - 445s 1s/step - loss: 0.0342 - accuracy: 0.9908 - macro_f1score: 0.2105 - weighted_f1score: 0.0039 - val_loss: 3.1757 - val_accuracy: 0.5744 - val_macro_f1score: 0.1133 - val_weighted_f1score: 0.0022\n",
      "Learning rate:  0.0001\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 70/70\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9911 - macro_f1score: 0.2100 - weighted_f1score: 0.0038\n",
      "Epoch 00070: val_loss did not improve from 2.18541\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.57439\n",
      "382/382 [==============================] - 442s 1s/step - loss: 0.0348 - accuracy: 0.9911 - macro_f1score: 0.2100 - weighted_f1score: 0.0038 - val_loss: 3.1672 - val_accuracy: 0.5744 - val_macro_f1score: 0.1133 - val_weighted_f1score: 0.0022\n"
     ]
    }
   ],
   "source": [
    "######## flow_from_directory\n",
    "history = model.fit(train_generator, steps_per_epoch=int(len(x_train)/batch_sizes),  validation_data = valid_generator, epochs=epochs , verbose=1 , callbacks = callbacks_list , validation_steps=int(len(x_valid)/batch_sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FAr6jLIR1qmu"
   },
   "source": [
    "### 2) DenseNet121 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 44782854,
     "status": "ok",
     "timestamp": 1599358397241,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "03303793760957673272"
     },
     "user_tz": -540
    },
    "id": "ZvKx1in81qmu",
    "outputId": "1f1699d9-bfae-45e9-c096-55cb9d684342",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 1200s 25s/step - loss: 3.2409 - accuracy: 0.5527 - macro_f1score: 0.1089 - weighted_f1score: 0.0021\n",
      "[Test Loss: 3.2409 /  Test Accuracy: 0.5527 / Test Macro f1: 0.1089 / Test Weighted f1: 0.0021]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. epoch=maximum\n",
    "loss , acc, mf1, wf1 = model.evaluate(test_generator,steps=int(len(x_test)/batch_sizes))\n",
    "print('[Test Loss: %.4f /  Test Accuracy: %.4f / Test Macro f1: %.4f / Test Weighted f1: %.4f]\\n' % (loss,acc,mf1,wf1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fbdRXP391qmx"
   },
   "outputs": [],
   "source": [
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "acc=history.history['accuracy']\n",
    "val_acc=history.history['val_accuracy']\n",
    "f1=history.history['macro_f1score']\n",
    "val_f1=history.history['val_macro_f1score']\n",
    "epochs=range(1,len(acc)+1)\n",
    "\n",
    "data = np.array([epochs,loss,val_loss,acc,val_acc,f1,val_f1]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gRtTWKDk1qm0"
   },
   "outputs": [],
   "source": [
    "# data save\n",
    "# epochs, loss, val_loss, acc, val_acc, f1, val_f1\n",
    "\n",
    "np.savetxt(os.path.join(dir,'train_valid_output',number,model.name+'.txt'),data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data import\n",
    "data = np.loadtxt(os.path.join(dir,'train_valid_output',number,'DenseNet121.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQVReEQO1qm5"
   },
   "outputs": [],
   "source": [
    "epochs=data[:,0]\n",
    "loss=data[:,1]\n",
    "val_loss=data[:,2]\n",
    "acc=data[:,3]\n",
    "val_acc=data[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kEBcd6Hx1qm8"
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs[1:],acc[1:],'b',label='Training Acc')\n",
    "plt.plot(epochs[1:],val_acc[1:],'r',label='Validation Acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs[10:],loss[10:],'b',label='Training Loss')\n",
    "plt.plot(epochs[10:],val_loss[10:],'r',label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jkbe4ajm1qnG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XLDgyUtp1qnJ"
   },
   "outputs": [],
   "source": [
    "# 이 아래는 내가 인위적으로 살펴보고 싶은 epoch에 대해서 결과값 출력 / DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2cgpzDgH1qnL"
   },
   "outputs": [],
   "source": [
    "model=load_model(os.path.join(dir,'model_output',number,'DenseNet121','026.h5'),custom_objects={\"macro_f1score\": macro_f1score,\"weighted_f1score\":weighted_f1score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I0IvhWkE1qnN"
   },
   "outputs": [],
   "source": [
    "# 2. epoch=?\n",
    "loss , acc, mf1, wf1 = model.evaluate(test_generator,steps=int(len(x_test)/batch_sizes))\n",
    "print('[Test Loss: %.4f /  Test Accuracy: %.4f / Test Macro f1: %.4f / Test Weighted f1: %.4f]\\n' % (loss,acc,mf1,wf1))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "11.DenseNet121.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
