{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vwVmRuFcUBkT"
   },
   "source": [
    "# [ResNet18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0MMz6DhUBkW"
   },
   "source": [
    "*KU LeeDongGyu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ywk25Fr79dWe"
   },
   "source": [
    "## Contents\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KSON5xiR9dWf"
   },
   "source": [
    "1. Data Preprocessing\n",
    "```\n",
    "1) Data Import\n",
    "2) Data Augmentation\n",
    "```\n",
    "2. Support Functions & Almost Original ResNet\n",
    "```\n",
    "1) Support Functions\n",
    "2) Almost orginal ResNet\n",
    "```\n",
    "3. ResNet18\n",
    "```\n",
    "1) ResNet18\n",
    "2) ResNet18 Evaluate\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U01q4o40UBkY"
   },
   "source": [
    "### Install Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XjdygsS_UBke"
   },
   "source": [
    "### Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53005,
     "status": "ok",
     "timestamp": 1599313787087,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "04369103463727261091"
     },
     "user_tz": -540
    },
    "id": "o1tpIlBhXG2i",
    "outputId": "4b82ccec-984e-4d93-ca7e-228488ce3c8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52975,
     "status": "ok",
     "timestamp": 1599313787092,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "04369103463727261091"
     },
     "user_tz": -540
    },
    "id": "IJdbC2nRXR6h",
    "outputId": "e2aa771b-faba-4bec-bf82-09ecb00c8aae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/Paper\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/My Drive/Colab Notebooks/Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I98omoQ8FF90"
   },
   "outputs": [],
   "source": [
    "from f1score import macro_f1score,weighted_f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKLMWqbuUBkf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as ks\n",
    "from tensorflow.keras import backend as K \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input, Concatenate, ZeroPadding2D ,GlobalMaxPooling2D, Reshape , Lambda , Add, Multiply\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, BatchNormalization, AveragePooling2D , ZeroPadding2D, SeparableConv2D\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop , SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping , LearningRateScheduler, ModelCheckpoint, CSVLogger, Callback, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l1,l2,l1_l2\n",
    "from tensorflow.keras.models import Model , load_model , Sequential\n",
    "from tensorflow.keras.utils import plot_model , to_categorical, get_file\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 55741,
     "status": "ok",
     "timestamp": 1599313790007,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "04369103463727261091"
     },
     "user_tz": -540
    },
    "id": "cbiFovMgXTax",
    "outputId": "fda023bb-9628-4e46-85b9-904238115c2a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/My Drive/Colab Notebooks/Paper'"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 55678,
     "status": "ok",
     "timestamp": 1599313790008,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "04369103463727261091"
     },
     "user_tz": -540
    },
    "id": "AcT0A_iwEzaZ",
    "outputId": "355fd203-bd1a-422a-cfb4-ddb8bc6079d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(ks.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 61209,
     "status": "ok",
     "timestamp": 1599313795573,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "04369103463727261091"
     },
     "user_tz": -540
    },
    "id": "Gr5hMHvmABmG",
    "outputId": "a61e9422-c062-4093-c426-2b6956573c22"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 61178,
     "status": "ok",
     "timestamp": 1599313795577,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "04369103463727261091"
     },
     "user_tz": -540
    },
    "id": "5QEPaq7XQBs8",
    "outputId": "302c24a1-a413-464f-9ae9-9d4a37f0f84a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7246986354754091418\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 9467238534618069420\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 7872252010289337056\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15695549568\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9591532391630351036\n",
      "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OOsm86eVUBko"
   },
   "source": [
    "## 1. Data Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DgwOtB_QEhll"
   },
   "source": [
    "### 1) Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i5hoO5oVDDJh"
   },
   "outputs": [],
   "source": [
    "# 바꿔서 살펴 볼 것들\n",
    "# CALTECH, CIFAR100, FER, MIT\n",
    "data_name = 'CALTECH'\n",
    "gan_type = 'No_GAN'\n",
    "number = '1'\n",
    "size = 224 # sizes after cropping\n",
    "super_size = 256 # sizes before cropping \n",
    "input_sizes = (size,size,3)\n",
    "batch_sizes = 128\n",
    "weight_decay = 3e-3\n",
    "epochs = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SPnWxfGzLOF6"
   },
   "outputs": [],
   "source": [
    "# 참고 : https://stackoverflow.com/questions/32419510/how-to-get-reproducible-results-in-keras/52897216#52897216\n",
    "# setting the seed number for random number generation for reproducibility.\n",
    "\n",
    "from numpy.random import seed\n",
    "import random\n",
    "\n",
    "\n",
    "if number=='1':\n",
    "    seed_num = 200225\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_num)\n",
    "    random.seed(seed_num)\n",
    "    seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "elif number=='2':\n",
    "    seed_num = 727\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_num)\n",
    "    random.seed(seed_num)\n",
    "    seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "elif number=='3':\n",
    "    seed_num = 115\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_num)\n",
    "    random.seed(seed_num)\n",
    "    seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "elif number=='4':\n",
    "    seed_num = 501\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_num)\n",
    "    random.seed(seed_num)\n",
    "    seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "elif number=='5':\n",
    "    seed_num = 517\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_num)\n",
    "    random.seed(seed_num)\n",
    "    seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qSIGjI-lnPzM"
   },
   "outputs": [],
   "source": [
    "# data import\n",
    "\n",
    "if data_name=='FER' :\n",
    "    x_train =  np.zeros(28698)\n",
    "    x_valid = np.zeros(3589)\n",
    "    x_test = np.zeros(3588)\n",
    "    classes = 7 \n",
    "    tr_center = [0.50793296, 0.50793296, 0.50793296]\n",
    "elif data_name=='MIT':\n",
    "    x_train = np.zeros(12466)\n",
    "    x_valid = np.zeros(1564)\n",
    "    x_test = np.zeros(1590)\n",
    "    classes = 67 \n",
    "    tr_center = [0.47916578, 0.42029615, 0.36046057]\n",
    "elif data_name=='CALTECH':\n",
    "    x_train = np.zeros(24510)\n",
    "    x_valid = np.zeros(2980)\n",
    "    x_test = np.zeros(3118)\n",
    "    classes = 257\n",
    "    tr_center = [0.51397761, 0.49525248, 0.46555727]\n",
    "elif data_name=='CIFAR100':\n",
    "    x_train = np.zeros(39941)\n",
    "    x_valid = np.zeros(10059)\n",
    "    x_test = np.zeros(10000)\n",
    "    classes = 100\n",
    "    tr_center = [0.53393271, 0.51324147, 0.46450563]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQlStjHWt-jM"
   },
   "outputs": [],
   "source": [
    "dir = os.path.join(os.getcwd(),data_name,gan_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XrNLBwoJCRR9"
   },
   "source": [
    "### 2) Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zs3J4oAbnPzR"
   },
   "outputs": [],
   "source": [
    "# 참고 : https://jkjung-avt.github.io/keras-image-cropping/\n",
    "\n",
    "def random_crop(img, random_crop_size):\n",
    "    # Note: image_data_format is 'channel_last'\n",
    "    assert img.shape[2] == 3 # img.shape[2] 가 3(rgb)이 아니면 assertion error 발생\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    dy, dx = random_crop_size\n",
    "    x = np.random.randint(0, width - dx + 1)\n",
    "    y = np.random.randint(0, height - dy + 1)\n",
    "    return img[y:(y+dy), x:(x+dx), :]\n",
    "\n",
    "\n",
    "def crop_generator(batches, crop_length):\n",
    "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
    "    crops from the image batches generated by the original iterator.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
    "        yield (batch_crops, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EW0KZ6x9EBtf"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array,load_img\n",
    "import glob\n",
    "\n",
    "# 데이터 전체에 대해 centering 진행함.\n",
    "\n",
    "def read_cal_image(img_path): \n",
    "    x = img_to_array(load_img(img_path)) # x는 채널별 평균값\n",
    "    y = x.shape[0] * x.shape[1]# y는 데이터별 픽셀 수 (비중)\n",
    "\n",
    "    x = 1/255. * x # scaling하고, centering값을 뽑아냄.\n",
    "    x = np.mean(x, axis=(0,1))\n",
    "    \n",
    "    return np.hstack([x,y])\n",
    "\n",
    "def calculate_centered_mean(dataset_path,x_train=x_train):\n",
    "    num = len(x_train)\n",
    "    space = np.empty((num,4))\n",
    "    i=0\n",
    "\n",
    "    for p in glob.glob(os.path.join(dataset_path,'*/*.*')) :\n",
    "        space[i] = read_cal_image(p)\n",
    "        i += 1\n",
    "\n",
    "    ratio = space[:,3] / np.sum(space[:,3])\n",
    "\n",
    "    return np.average(space[:,0:3],axis=0,weights=ratio)\n",
    "\n",
    "\n",
    "# 아래의 함수를 돌려서 나온 결과값을 중심화 값으로 설정.\n",
    "\n",
    "# train_mean = calculate_centered_mean(os.path.join(dir,'data/train')).reshape((1,1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vOCgiW4fnPzW"
   },
   "outputs": [],
   "source": [
    "datagen_tr = ImageDataGenerator(\n",
    "    rescale=1/255.,\n",
    "    horizontal_flip=True,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=False,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=[0.9,1.0],\n",
    "    fill_mode = 'nearest')\n",
    "datagen_val = ImageDataGenerator(rescale=1/255.,featurewise_center=True)\n",
    "datagen_tes = ImageDataGenerator(rescale=1/255.,featurewise_center=True)\n",
    "\n",
    "# 원래는 이 자리에 fit 매서드를 써야하지만, 그냥 내가 중심화함수를 만들고 적용함. \n",
    "\n",
    "# 중심화 설정\n",
    "datagen_tr.mean = np.array(tr_center, dtype=np.float32).reshape((1,1,3)) # RGB\n",
    "datagen_val.mean = np.array(tr_center, dtype=np.float32).reshape((1,1,3)) # RGB\n",
    "datagen_tes.mean = np.array(tr_center, dtype=np.float32).reshape((1,1,3)) # RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 119393,
     "status": "ok",
     "timestamp": 1599313853938,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "04369103463727261091"
     },
     "user_tz": -540
    },
    "id": "sh3c_SsjnPzY",
    "outputId": "567c9420-8e37-4a71-e393-cce4c10fa077"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24509 images belonging to 257 classes.\n",
      "Found 2980 images belonging to 257 classes.\n",
      "Found 3118 images belonging to 257 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = datagen_tr.flow_from_directory(directory=os.path.join(dir,'data/train'),target_size=(super_size,super_size),batch_size=batch_sizes,class_mode='categorical') # fer : 28698 / mit : 12466 / caltech : 24509 / cifar : 39941\n",
    "train_generator= crop_generator(train_batches, size)\n",
    "valid_generator = datagen_val.flow_from_directory(directory=os.path.join(dir,'data/valid'),target_size=(size,size),batch_size=batch_sizes,class_mode='categorical') # fer : 3589 / mit : 1564 / caltech : 2980 / cifar : 10059\n",
    "test_generator = datagen_tes.flow_from_directory(directory=os.path.join(dir,'data/test'),target_size=(size,size),batch_size=batch_sizes,class_mode='categorical') # fer : 3588 / mit : 1590 / caltech : 3118 / cifar : 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "afa9Vvwdw1te"
   },
   "source": [
    "## 2. Support Functions & Almost Original ResNet\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iER-OGdBw1tf"
   },
   "source": [
    "### 1) Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n2aVgcbl5z0A"
   },
   "outputs": [],
   "source": [
    "# def lr_schedule(epoch):\n",
    "#     init_lr = 1e-4\n",
    "#     k = 0.04\n",
    "#     lr = init_lr * np.exp(-k*epoch)\n",
    "#     print('Learning rate: ', lr)\n",
    "#     return lr\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch < 60:\n",
    "        lr = lr\n",
    "    else :\n",
    "        lr = lr * 0.1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TjxgKIgDw1tj"
   },
   "outputs": [],
   "source": [
    "# 편의상 아래와 같은 resnet 묶음의 이름을 resnet_layer이라 하자 \n",
    "def resnet_layer(inputs, weight_decay=weight_decay, num_filters=16, kernel_size=(3,3), strides=(1,1),activation=None):\n",
    "\n",
    "    conv = Conv2D(num_filters,kernel_size=kernel_size,strides=strides, padding='same', kernel_initializer='he_uniform', kernel_regularizer=l2(weight_decay))\n",
    "    x = inputs\n",
    "    x = conv(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if activation is not None:\n",
    "        x = Activation(activation)(x) # conv-bn-activation\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u8Ss5eT5w1tm"
   },
   "outputs": [],
   "source": [
    "# resnet 코드를 만들기 앞서, 코드가 너무 길어 비효율적이므로 적당한 함수를 만든다.\n",
    "# block_v1 은 resnet18과 resnet34에 특화된 block이고, block_v2는 resnet50, resnet101, resnet152에 특화된 block이다.\n",
    "# 이 두개의 block은, 나중에 하나의 모형에서 같이 고려될 것이다.\n",
    "\n",
    "def block_v1(input_shape, num_filters, identi, half=True):\n",
    "    # identi : resnet의 존재 이유인, 더하게 될 자기자신값이다.\n",
    "    if half:\n",
    "        layer1 = resnet_layer(inputs=input_shape, num_filters=num_filters, strides=(2, 2), activation='relu')\n",
    "        layer2 = resnet_layer(inputs=layer1, num_filters=num_filters, strides=(1, 1), activation=None)\n",
    "    else:\n",
    "        layer1 = resnet_layer(inputs=input_shape, num_filters=num_filters, strides=(1, 1), activation='relu')\n",
    "        layer2 = resnet_layer(inputs=layer1, num_filters=num_filters, strides=(1, 1), activation=None)\n",
    "    \n",
    "    res_2_1 = add([identi, layer2])  # block 1\n",
    "    output = Activation(\"relu\")(res_2_1)\n",
    "    return output\n",
    "\n",
    "\n",
    "def block_v2(input_shape, num_filters, identi, half=True):\n",
    "    # identi : resnet의 존재 이유인, 더하게 될 자기자신값이다.\n",
    "    if half:\n",
    "        layer1 = resnet_layer(inputs=input_shape, num_filters=num_filters, strides=(2, 2), kernel_size=(1, 1), activation='relu')\n",
    "        layer2 = resnet_layer(inputs=layer1, num_filters=num_filters, strides=(1, 1), kernel_size=(3, 3), activation='relu')\n",
    "        layer3 = resnet_layer(inputs=layer2, num_filters= 4*num_filters, strides=(1, 1), kernel_size=(1, 1), activation=None)\n",
    "    else:\n",
    "        layer1 = resnet_layer(inputs=input_shape, num_filters= num_filters, strides=(1, 1), kernel_size=(1, 1), activation='relu')\n",
    "        layer2 = resnet_layer(inputs=layer1, num_filters= num_filters, strides=(1, 1), kernel_size=(3, 3), activation='relu')\n",
    "        layer3 = resnet_layer(inputs=layer2, num_filters= 4*num_filters, strides=(1, 1), kernel_size=(1, 1), activation=None)\n",
    "\n",
    "    res_3_1 = add([identi, layer3])  # block 1\n",
    "    output = Activation(\"relu\")(res_3_1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ikmjp50w1uA"
   },
   "source": [
    "### 2) Almost Orginial ResNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGdsPCI1w1uB"
   },
   "outputs": [],
   "source": [
    "# 좀더 깔끔하게 작성됨. 앞으로의 resnet모형은 다음의 함수에서 해결한다.\n",
    "def ResNet(input_shape=(224, 224, 3), classes=1000, weight_decay=weight_decay, num_filters=64, num_blocks=[0,0,0,0] ,version=None, name=None):\n",
    "    # num_blocks 는 list로 받는 인자. conv3_x ~ conv5_x의 block의 수를 앞에서부터 차례로 넣어주면 된다.\n",
    "    # num_filters 는 초기의 filters를 의미한다.\n",
    "    \n",
    "    if len(num_blocks) != 4 :\n",
    "        raise NameError(\"Please input the number of blocks from conv2_x to conv5_x in the 'num_blocks' variable.\")\n",
    "\n",
    "    if version == \"v1\" : # resnet 18, 34에 해당. (블록이 같음)\n",
    "        block = block_v1\n",
    "    elif version == \"v2\": # resnet 50, 101, 152에 해당. (블록이 같음)\n",
    "        block = block_v2\n",
    "    else :\n",
    "        raise NameError(\"Please input the string 'v1' or 'v2' in the 'version' variable. \")\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # conv1\n",
    "    layer1 = resnet_layer(inputs=inputs, num_filters=num_filters, strides=(2, 2), kernel_size=(7, 7), activation='relu')\n",
    "    cum_block = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same', name='pool1/3x3_s2')(layer1)\n",
    "    \n",
    "    # cum_block 변수에 하나씩 residual block을 누적시켜보자.\n",
    "\n",
    "    for stack in range(4):\n",
    "        if stack == 0 :\n",
    "            # 우리가 누적시킬 변수, cum_block을 정의하고, 이 변수에 누적시켜가면서 모형을 출력한다.\n",
    "\n",
    "            # 이 아래 for문은 conv2_x\n",
    "            # 들어가기 전에, block_v2라면 input과 output의 filter size를 동일하게 해주는 작업을 한다. 덧셈이 가능해야 하므로, 반드시!\n",
    "            if version == 'v2':\n",
    "                x = Conv2D(4 * num_filters, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_initializer='he_uniform', kernel_regularizer=l2(weight_decay))(cum_block)\n",
    "                x = BatchNormalization()(x) # 모든 conv.뒤에는 bn을 적용.\n",
    "                cum_block = block(input_shape=cum_block, num_filters=num_filters, identi=x, half=False)\n",
    "\n",
    "            if version != 'v2':\n",
    "                  new_num_blocks = num_blocks[stack]\n",
    "            else :\n",
    "                  new_num_blocks = num_blocks[stack]-1\n",
    "\n",
    "            for res_block in range(new_num_blocks):\n",
    "                cum_block = block(input_shape=cum_block, num_filters=num_filters, identi=cum_block, half=False)\n",
    "        else:\n",
    "\n",
    "            # 이 아래는 conv3_x ~ conv5_x 까지.\n",
    "            num_filters *= 2\n",
    " \n",
    "            if version == 'v2':\n",
    "                x = Conv2D(4 * num_filters, kernel_size=(1, 1), strides=(2, 2), padding='valid', kernel_initializer='he_uniform', kernel_regularizer=l2(weight_decay))(cum_block)  # block_v2라면, input에 해당하는 x를 덧셈이 가능하게 맞춰준다.\n",
    "                x = BatchNormalization()(x) # 모든 conv.뒤에는 bn을 적용.\n",
    "            else:\n",
    "                x = Conv2D(num_filters, (1, 1), strides=(2, 2), padding='valid', kernel_initializer='he_uniform', kernel_regularizer=l2(weight_decay))(cum_block)  # identi_input 에 해당 // rxc가 각각 반토막남.\n",
    "                x = BatchNormalization()(x) # 모든 conv.뒤에는 bn을 적용.\n",
    "                #즉, 위 x는 결국 이어질 conv 층의 input임.\n",
    "\n",
    "            for res_block in range(num_blocks[stack]):\n",
    "                if res_block == 0:\n",
    "                    cum_block = block(input_shape=cum_block , num_filters = num_filters , identi=x, half=True) # block 갱신\n",
    "                else:\n",
    "                    cum_block = block(input_shape=cum_block , num_filters = num_filters , identi=cum_block , half=False) # block 갱신\n",
    "\n",
    "    # 마지막 1층\n",
    "    y = GlobalAveragePooling2D()(cum_block)\n",
    "    outputs = Dense(classes, activation='softmax')(y)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name = name)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tOAp8LsWRMR6"
   },
   "source": [
    "## 3. ResNet18\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "18V3G5o-RMR7"
   },
   "source": [
    "### 1) ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V9CwzBHWRMR8"
   },
   "outputs": [],
   "source": [
    "# ResNet18\n",
    "model = ResNet(input_shape=input_sizes, classes=classes, num_blocks=[2,2,2,2], version='v1', name='ResNet18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 120284,
     "status": "ok",
     "timestamp": 1599313854969,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "04369103463727261091"
     },
     "user_tz": -540
    },
    "id": "u9Lro6eaRMSC",
    "outputId": "62f23757-1ee8-48b3-902c-813d3842b156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet18\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 112, 112, 64) 9472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 112, 112, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 112, 112, 64) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool1/3x3_s2 (MaxPooling2D)     (None, 56, 56, 64)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 56, 56, 64)   36928       pool1/3x3_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 56, 56, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 56, 56, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 56, 56, 64)   36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 56, 56, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 56, 56, 64)   0           pool1/3x3_s2[0][0]               \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 56, 56, 64)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 56, 56, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 56, 56, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 56, 56, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 56, 56, 64)   36928       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 56, 56, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 56, 56, 64)   0           activation_2[0][0]               \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 56, 56, 64)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 28, 28, 128)  73856       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 28, 28, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 28, 28, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 28, 28, 128)  8320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 28, 28, 128)  147584      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 28, 28, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 28, 28, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 28, 28, 128)  0           batch_normalization_5[0][0]      \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 28, 28, 128)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 28, 28, 128)  147584      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 28, 28, 128)  512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 28, 28, 128)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 28, 28, 128)  147584      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 28, 28, 128)  512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 28, 28, 128)  0           activation_6[0][0]               \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 28, 28, 128)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 14, 14, 256)  295168      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 14, 14, 256)  1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 14, 14, 256)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 14, 14, 256)  33024       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 14, 14, 256)  590080      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 14, 14, 256)  1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 14, 14, 256)  1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 14, 14, 256)  0           batch_normalization_10[0][0]     \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 14, 14, 256)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 14, 14, 256)  590080      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 14, 14, 256)  1024        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 14, 14, 256)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 14, 14, 256)  590080      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 14, 14, 256)  1024        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 14, 14, 256)  0           activation_10[0][0]              \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 14, 14, 256)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 7, 7, 512)    1180160     activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 7, 7, 512)    2048        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 7, 7, 512)    0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 7, 7, 512)    131584      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 7, 7, 512)    2359808     activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 7, 7, 512)    2048        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 7, 7, 512)    2048        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 7, 7, 512)    0           batch_normalization_15[0][0]     \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 7, 7, 512)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 7, 7, 512)    2359808     activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 7, 7, 512)    2048        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 7, 7, 512)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 7, 7, 512)    2359808     activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 7, 7, 512)    2048        conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 7, 7, 512)    0           activation_14[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 7, 7, 512)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 512)          0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 257)          131841      global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 11,322,753\n",
      "Trainable params: 11,313,153\n",
      "Non-trainable params: 9,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AispA2HmDSs_"
   },
   "outputs": [],
   "source": [
    "# 폴더 생성\n",
    "\n",
    "os.makedirs(os.path.join(dir,'model_output',number,model.name), exist_ok=True)\n",
    "os.makedirs(os.path.join(dir,'train_valid_output',number), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e4TEeLLw9al1"
   },
   "outputs": [],
   "source": [
    "# 참고 : https://github.com/OverLordGoldDragon/keras-adamw\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/Colab Notebooks/Paper')\n",
    "import utils\n",
    "import optimizers_v2\n",
    "from utils import get_weight_decays, fill_dict_in_order\n",
    "from utils import reset_seeds, K_eval\n",
    "from optimizers_v2 import AdamW, NadamW, SGDW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 121726,
     "status": "ok",
     "timestamp": 1599313856519,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "04369103463727261091"
     },
     "user_tz": -540
    },
    "id": "rwH0Q16iRMSH",
    "outputId": "cee45892-318e-4aa3-ab5a-08418393e416"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cosine annealing learning rates\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model=model, use_cosine_annealing=True, total_iterations = len(x_train) // batch_sizes , eta_min = 1e-2)\n",
    "#optimizer = Adam()\n",
    "filepath =  os.path.join(dir,'model_output',number,model.name,'{epoch:03d}.h5')\n",
    "\n",
    "callbacks_list = [ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_weights_only=False, save_best_only=True, mode='min'),\n",
    "                  ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_weights_only=False, save_best_only=True, mode='max'),\n",
    "                  #ReduceLROnPlateau(monitor='val_loss',patience=3,factor=0.1,min_lr=1e-5),\n",
    "                  LearningRateScheduler(lr_schedule,verbose=1)\n",
    "                  ]\n",
    "                  \n",
    "model.compile(optimizer, loss = 'categorical_crossentropy', metrics=['accuracy',macro_f1score,weighted_f1score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## flow_from_directory\n",
    "history = model.fit(train_generator, steps_per_epoch=int(len(x_train)/batch_sizes),  validation_data = valid_generator, epochs=epochs , verbose=1 , callbacks = callbacks_list , validation_steps=int(len(x_valid)/batch_sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6J7qM483RMST"
   },
   "source": [
    "### 2) ResNet18 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. epoch=maximum\n",
    "loss , acc, mf1, wf1 = model.evaluate(test_generator,steps=int(len(x_test)/batch_sizes))\n",
    "print('[Test Loss: %.4f /  Test Accuracy: %.4f / Test Macro f1: %.4f / Test Weighted f1: %.4f]\\n' % (loss,acc,mf1,wf1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mWbV6MR0RMSY"
   },
   "outputs": [],
   "source": [
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "acc=history.history['accuracy']\n",
    "val_acc=history.history['val_accuracy']\n",
    "f1=history.history['macro_f1score']\n",
    "val_f1=history.history['val_macro_f1score']\n",
    "epochs=range(1,len(acc)+1)\n",
    "\n",
    "data = np.array([epochs,loss,val_loss,acc,val_acc,f1,val_f1]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_HOf2qVRMSe"
   },
   "outputs": [],
   "source": [
    "# data save\n",
    "# epochs, loss, val_loss, acc, val_acc, f1, val_f1\n",
    "\n",
    "np.savetxt(os.path.join(dir,'train_valid_output',number,model.name+'.txt'),data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DLCKPSirRMSi"
   },
   "outputs": [],
   "source": [
    "# data import\n",
    "data = np.loadtxt(os.path.join(dir,'train_valid_output',number,'ResNet18.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OWoU3mT7RMSn"
   },
   "outputs": [],
   "source": [
    "epochs=data[:,0]\n",
    "loss=data[:,1]\n",
    "val_loss=data[:,2]\n",
    "acc=data[:,3]\n",
    "val_acc=data[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs[1:],acc[1:],'b',label='Training Acc')\n",
    "plt.plot(epochs[1:],val_acc[1:],'r',label='Validation Acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs[10:],loss[10:],'b',label='Training Loss')\n",
    "plt.plot(epochs[10:],val_loss[10:],'r',label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IA98syIrYvyu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a0z6BwcAulnw"
   },
   "outputs": [],
   "source": [
    "# 이 아래는 내가 인위적으로 살펴보고 싶은 epoch에 대해서 결과값 출력 / resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model(os.path.join(dir,'model_output',number,'ResNet18','031.h5'),custom_objects={\"macro_f1score\": macro_f1score,\"weighted_f1score\":weighted_f1score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VWlrnY9EuDG-"
   },
   "outputs": [],
   "source": [
    "# 2. epoch=?\n",
    "loss , acc, mf1, wf1 = model.evaluate(test_generator,steps=int(len(x_test)/batch_sizes))\n",
    "print('[Test Loss: %.4f /  Test Accuracy: %.4f / Test Macro f1: %.4f / Test Weighted f1: %.4f]\\n' % (loss,acc,mf1,wf1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8R2IzySaD90j"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "4.ResNet18.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
