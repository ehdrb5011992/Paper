{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vwVmRuFcUBkT"
   },
   "source": [
    "# [SE-ResNext18]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0MMz6DhUBkW"
   },
   "source": [
    "*KU LeeDongGyu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ywk25Fr79dWe"
   },
   "source": [
    "## Contents\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KSON5xiR9dWf"
   },
   "source": [
    "1. Data Preprocessing\n",
    "```\n",
    "1) Data Import\n",
    "2) Data Augmentation\n",
    "```\n",
    "2. Support Functions & Almost Original SE-ResNext18\n",
    "```\n",
    "1) Support Functions\n",
    "2) Almost orginal SE-ResNext18\n",
    "```\n",
    "3. SE-ResNext18\n",
    "```\n",
    "1) SE-ResNext18\n",
    "2) SE-ResNext18 Evaluate\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U01q4o40UBkY"
   },
   "source": [
    "### Install Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XjdygsS_UBke"
   },
   "source": [
    "### Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 99274,
     "status": "ok",
     "timestamp": 1599313718531,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "16331884730664529787"
     },
     "user_tz": -540
    },
    "id": "o1tpIlBhXG2i",
    "outputId": "46acbd4c-3a0d-455a-fd60-cf8bfac62413"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 99255,
     "status": "ok",
     "timestamp": 1599313718534,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "16331884730664529787"
     },
     "user_tz": -540
    },
    "id": "IJdbC2nRXR6h",
    "outputId": "a8874e12-6b91-42a8-9a52-5513a0ae673c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/Paper\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/My Drive/Colab Notebooks/Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I98omoQ8FF90"
   },
   "outputs": [],
   "source": [
    "from f1score import macro_f1score,weighted_f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKLMWqbuUBkf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as ks\n",
    "from tensorflow.keras import backend as K \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input, Concatenate, ZeroPadding2D ,GlobalMaxPooling2D, Reshape , Lambda , Add, Multiply\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, BatchNormalization, AveragePooling2D , ZeroPadding2D, SeparableConv2D\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop , SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping , LearningRateScheduler, ModelCheckpoint, CSVLogger, Callback, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l1,l2,l1_l2\n",
    "from tensorflow.keras.models import Model , load_model , Sequential\n",
    "from tensorflow.keras.utils import plot_model , to_categorical, get_file\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 102163,
     "status": "ok",
     "timestamp": 1599313721475,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "16331884730664529787"
     },
     "user_tz": -540
    },
    "id": "cbiFovMgXTax",
    "outputId": "31278805-186e-42b1-9513-a5f336549fdb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/My Drive/Colab Notebooks/Paper'"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 102155,
     "status": "ok",
     "timestamp": 1599313721476,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "16331884730664529787"
     },
     "user_tz": -540
    },
    "id": "AcT0A_iwEzaZ",
    "outputId": "655add93-3e6c-455b-c70e-def1f03cf4da"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 107964,
     "status": "ok",
     "timestamp": 1599313727301,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "16331884730664529787"
     },
     "user_tz": -540
    },
    "id": "Gr5hMHvmABmG",
    "outputId": "26ecc60b-5152-4722-e16f-03892ca637c2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 107946,
     "status": "ok",
     "timestamp": 1599313727302,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "16331884730664529787"
     },
     "user_tz": -540
    },
    "id": "Kf057Rw2yigs",
    "outputId": "dcfad6a4-105d-419a-9146-8c199441b794"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2510590179544387677\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 8328551092851856242\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 15330411255970660508\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15695549568\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8398308428305062880\n",
      "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sTXnS6_magkg"
   },
   "source": [
    "## 1. Data Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FWigHOuzCRRj"
   },
   "source": [
    "### 1) Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BeJ7UtuOEgWY"
   },
   "outputs": [],
   "source": [
    "# 바꿔서 살펴 볼 것들\n",
    "# CALTECH, CIFAR100, FER, MIT\n",
    "data_name = 'CALTECH'\n",
    "gan_type = 'No_GAN'\n",
    "number = '1'\n",
    "size = 224 # sizes after cropping\n",
    "super_size = 256 # sizes before cropping \n",
    "input_sizes = (size,size,3)\n",
    "batch_sizes = 64\n",
    "weight_decay = 3e-3     #resnet18은 3e-4 , se-resnet18은 5e-4\n",
    "epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SPnWxfGzLOF6"
   },
   "outputs": [],
   "source": [
    "# 참고 : https://stackoverflow.com/questions/32419510/how-to-get-reproducible-results-in-keras/52897216#52897216\n",
    "# setting the seed number for random number generation for reproducibility.\n",
    "\n",
    "from numpy.random import seed\n",
    "import random\n",
    "\n",
    "\n",
    "if number=='1':\n",
    "    seed_num = 200225\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_num)\n",
    "    random.seed(seed_num)\n",
    "    seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "elif number=='2':\n",
    "    seed_num = 727\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_num)\n",
    "    random.seed(seed_num)\n",
    "    seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "elif number=='3':\n",
    "    seed_num = 115\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_num)\n",
    "    random.seed(seed_num)\n",
    "    seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "elif number=='4':\n",
    "    seed_num = 501\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_num)\n",
    "    random.seed(seed_num)\n",
    "    seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "elif number=='5':\n",
    "    seed_num = 517\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_num)\n",
    "    random.seed(seed_num)\n",
    "    seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aWc_XOgNpPlL"
   },
   "outputs": [],
   "source": [
    "# data import\n",
    "\n",
    "if data_name=='FER' :\n",
    "    x_train =  np.zeros(28698)\n",
    "    x_valid = np.zeros(3589)\n",
    "    x_test = np.zeros(3588)\n",
    "    classes = 7 \n",
    "    tr_center = [0.50793296, 0.50793296, 0.50793296]\n",
    "elif data_name=='MIT':\n",
    "    x_train = np.zeros(12466)\n",
    "    x_valid = np.zeros(1564)\n",
    "    x_test = np.zeros(1590)\n",
    "    classes = 67 \n",
    "    tr_center = [0.47916578, 0.42029615, 0.36046057]\n",
    "elif data_name=='CALTECH':\n",
    "    x_train = np.zeros(24510)\n",
    "    x_valid = np.zeros(2980)\n",
    "    x_test = np.zeros(3118)\n",
    "    classes = 257\n",
    "    tr_center = [0.51397761, 0.49525248, 0.46555727]\n",
    "elif data_name=='CIFAR100':\n",
    "    x_train = np.zeros(39941)\n",
    "    x_valid = np.zeros(10059)\n",
    "    x_test = np.zeros(10000)\n",
    "    classes = 100\n",
    "    tr_center = [0.53393271, 0.51324147, 0.46450563]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQlStjHWt-jM"
   },
   "outputs": [],
   "source": [
    "dir = os.path.join(os.getcwd(),data_name,gan_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vuLyOxlopPlV"
   },
   "source": [
    "### 2) Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sH3Y2hZBpPlV"
   },
   "outputs": [],
   "source": [
    "# 참고 : https://jkjung-avt.github.io/keras-image-cropping/\n",
    "\n",
    "def random_crop(img, random_crop_size):\n",
    "    # Note: image_data_format is 'channel_last'\n",
    "    assert img.shape[2] == 3 # img.shape[2] 가 3(rgb)이 아니면 assertion error 발생\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    dy, dx = random_crop_size\n",
    "    x = np.random.randint(0, width - dx + 1)\n",
    "    y = np.random.randint(0, height - dy + 1)\n",
    "    return img[y:(y+dy), x:(x+dx), :]\n",
    "\n",
    "\n",
    "def crop_generator(batches, crop_length):\n",
    "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
    "    crops from the image batches generated by the original iterator.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
    "        yield (batch_crops, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EW0KZ6x9EBtf"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array,load_img\n",
    "import glob\n",
    "\n",
    "# 데이터 전체에 대해 centering 진행함.\n",
    "\n",
    "def read_cal_image(img_path): \n",
    "    x = img_to_array(load_img(img_path)) # x는 채널별 평균값\n",
    "    y = x.shape[0] * x.shape[1]# y는 데이터별 픽셀 수 (비중)\n",
    "\n",
    "    x = 1/255. * x # scaling하고, centering값을 뽑아냄.\n",
    "    x = np.mean(x, axis=(0,1))\n",
    "    \n",
    "    return np.hstack([x,y])\n",
    "\n",
    "def calculate_centered_mean(dataset_path,x_train=x_train):\n",
    "    num = len(x_train)\n",
    "    space = np.empty((num,4))\n",
    "    i=0\n",
    "\n",
    "    for p in glob.glob(os.path.join(dataset_path,'*/*.*')) :\n",
    "        space[i] = read_cal_image(p)\n",
    "        i += 1\n",
    "\n",
    "    ratio = space[:,3] / np.sum(space[:,3])\n",
    "\n",
    "    return np.average(space[:,0:3],axis=0,weights=ratio)\n",
    "\n",
    "\n",
    "# 아래의 함수를 돌려서 나온 결과값을 중심화 값으로 설정.\n",
    "\n",
    "# train_mean = calculate_centered_mean(os.path.join(dir,'data/train')).reshape((1,1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a2JcGYwcpPla"
   },
   "outputs": [],
   "source": [
    "datagen_tr = ImageDataGenerator(\n",
    "    rescale=1/255.,\n",
    "    horizontal_flip=True,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=False,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=[0.9,1.0],\n",
    "    fill_mode = 'nearest')\n",
    "datagen_val = ImageDataGenerator(rescale=1/255.,featurewise_center=True)\n",
    "datagen_tes = ImageDataGenerator(rescale=1/255.,featurewise_center=True)\n",
    "\n",
    "# 원래는 이 자리에 fit 매서드를 써야하지만, 그냥 내가 중심화함수를 만들고 적용함. \n",
    "\n",
    "# 중심화 설정\n",
    "datagen_tr.mean = np.array(tr_center, dtype=np.float32).reshape((1,1,3)) # RGB\n",
    "datagen_val.mean = np.array(tr_center, dtype=np.float32).reshape((1,1,3)) # RGB\n",
    "datagen_tes.mean = np.array(tr_center, dtype=np.float32).reshape((1,1,3)) # RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 174937,
     "status": "ok",
     "timestamp": 1599313794377,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "16331884730664529787"
     },
     "user_tz": -540
    },
    "id": "4fStY438pPlb",
    "outputId": "d6ce39ff-bc32-4602-a04e-d8a8b9b19742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24509 images belonging to 257 classes.\n",
      "Found 2980 images belonging to 257 classes.\n",
      "Found 3118 images belonging to 257 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = datagen_tr.flow_from_directory(directory=os.path.join(dir,'data/train'),target_size=(super_size,super_size),batch_size=batch_sizes,class_mode='categorical') # fer : 28698 / mit : 12466 / caltech : 24509 / cifar : 39941\n",
    "train_generator= crop_generator(train_batches, size)\n",
    "valid_generator = datagen_val.flow_from_directory(directory=os.path.join(dir,'data/valid'),target_size=(size,size),batch_size=batch_sizes,class_mode='categorical') # fer : 3589 / mit : 1564 / caltech : 2980 / cifar : 10059\n",
    "test_generator = datagen_tes.flow_from_directory(directory=os.path.join(dir,'data/test'),target_size=(size,size),batch_size=batch_sizes,class_mode='categorical') # fer : 3588 / mit : 1590 / caltech : 3118 / cifar : 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gn_DwVVV1qmT"
   },
   "source": [
    "## 2. Support Functions & Almost Original SE-ResNext\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GrcgSgWh1qmT"
   },
   "source": [
    "### 1) Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XA3sqFv_ZyBS"
   },
   "outputs": [],
   "source": [
    "# def lr_schedule(epoch):\n",
    "#     init_lr = 1e-4\n",
    "#     k = 0.04\n",
    "#     lr = init_lr * np.exp(-k*epoch)\n",
    "#     print('Learning rate: ', lr)\n",
    "#     return lr\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch < 50:\n",
    "        lr = lr\n",
    "    else :\n",
    "        lr = lr * 0.1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RdgVZoWXZr_e"
   },
   "outputs": [],
   "source": [
    "# SE block을 만들고 붙이자.\n",
    "def SE_block(input_tensor,reduction_ratio):\n",
    "    ch_input = K.int_shape(input_tensor)[-1]\n",
    "    ch_reduced = ch_input//reduction_ratio\n",
    "    \n",
    "    # Squeeze\n",
    "    x = GlobalAveragePooling2D()(input_tensor) \n",
    "    \n",
    "    # Excitation\n",
    "    x = Dense(ch_reduced, activation='relu', kernel_initializer='he_uniform', use_bias=False)(x) \n",
    "    x = Dense(ch_input, activation='sigmoid', use_bias=False)(x) \n",
    "    \n",
    "    x = Reshape( (1, 1, ch_input) )(x)\n",
    "    x = Multiply()([input_tensor, x]) # broadcasting \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uY2LEhqW1qmX"
   },
   "outputs": [],
   "source": [
    "def resnext_layer(inputs, weight_decay=weight_decay, num_filters=16, kernel_size=(3,3), strides=(1,1),activation=None):\n",
    "\n",
    "    conv = Conv2D(num_filters,kernel_size=kernel_size,strides=strides, padding='same', kernel_initializer='he_uniform', kernel_regularizer=l2(weight_decay))\n",
    "    x = inputs\n",
    "    x = conv(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if activation is not None:\n",
    "        x = Activation(activation)(x) # conv-bn-activation\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5IqNGAIi1qmZ"
   },
   "outputs": [],
   "source": [
    "def block_v1(input_shape, identi, num_filters, reduction_ratio, half=True, C = 32):\n",
    "\n",
    "    layer1=[0]*C\n",
    "    layer2=[0]*C\n",
    "\n",
    "    if half:\n",
    "        for i in range(C) :\n",
    "            layer1[i] = resnext_layer(input_shape, num_filters=num_filters // C, strides=(2, 2), activation='relu')\n",
    "            layer2[i] = resnext_layer(layer1[i], num_filters=num_filters // 2 , strides=(1, 1), activation=None)\n",
    "    else:\n",
    "        for i in range(C) :\n",
    "            layer1[i] = resnext_layer(input_shape, num_filters=num_filters //C, strides=(1, 1), activation='relu')\n",
    "            layer2[i] = resnext_layer(layer1[i], num_filters=num_filters // 2, strides=(1, 1), activation=None)\n",
    "\n",
    "\n",
    "    res_2_1 = add(layer2)\n",
    "    \n",
    "    # SE block 추가\n",
    "    res_2_1 =  SE_block(res_2_1, reduction_ratio)\n",
    "\n",
    "    res_2_2 = add([identi, res_2_1])  # block 2\n",
    "    output = Activation(\"relu\")(res_2_2)\n",
    "    return output\n",
    "\n",
    "\n",
    "def block_v2(input_shape, identi, num_filters, reduction_ratio, half=True , C = 32):\n",
    "    \n",
    "    layer1=[0]*C\n",
    "    layer2=[0]*C\n",
    "    layer3=[0]*C\n",
    "\n",
    "    if half:\n",
    "        for i in range(C):\n",
    "            layer1[i] = resnext_layer(input_shape, num_filters=num_filters // C, kernel_size=(1, 1), strides=(2, 2), activation='relu')\n",
    "            layer2[i] = resnext_layer(layer1[i], num_filters=num_filters // C, kernel_size=(3, 3), strides=(1, 1), activation='relu')\n",
    "            layer3[i] = resnext_layer(layer2[i], num_filters= num_filters * 2, kernel_size=(1, 1), strides=(1, 1), activation=None)\n",
    "    else:\n",
    "        for i in range(C):\n",
    "            layer1[i] = resnext_layer(input_shape, num_filters= num_filters // C, kernel_size=(1, 1), strides=(1, 1), activation='relu')\n",
    "            layer2[i] = resnext_layer(layer1[i], num_filters= num_filters // C, kernel_size=(3, 3), strides=(1, 1), activation='relu')\n",
    "            layer3[i] = resnext_layer(layer2[i], num_filters= num_filters * 2, kernel_size=(1, 1), strides=(1, 1), activation=None)\n",
    "\n",
    "\n",
    "    res_3_1 = add(layer3)\n",
    "    \n",
    "    # SE block 추가\n",
    "    res_3_1 =  SE_block(res_3_1, reduction_ratio)\n",
    "\n",
    "    res_3_2 = add([identi, res_3_1])  # block 2\n",
    "    output = Activation(\"relu\")(res_3_2)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Lho4zoe1qmc"
   },
   "source": [
    "### 2) Almost Orginial SE-ResNext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q_KafJBn1qmd"
   },
   "outputs": [],
   "source": [
    "def SE_ResNext(input_shape=(224, 224, 3), weight_decay=weight_decay, num_filters=128, C = 32, reduction_ratio=16, classes=1000, num_blocks=[0,0,0,0] ,version=None, name=None ):\n",
    "    # num_blocks 는 list로 받는 인자. conv3_x ~ conv5_x의 block의 수를 앞에서부터 차례로 넣어주면 된다.\n",
    "    # num_filters 는 초기의 filters를 의미한다.\n",
    "    \n",
    "    if len(num_blocks) != 4 :\n",
    "        raise NameError(\"Please input the number of blocks from conv2_x to conv5_x in the 'num_blocks' variable.\")\n",
    "\n",
    "    if version == \"v1\" : \n",
    "        block = block_v1\n",
    "    elif version == \"v2\": \n",
    "        block = block_v2\n",
    "    else :\n",
    "        raise NameError(\"Please input the string 'v1' or 'v2' in the 'version' variable. \")\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    layer1 = resnext_layer(inputs, num_filters=num_filters, strides=(2, 2), kernel_size=(7, 7), activation='relu')\n",
    "    cum_block = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same', name='pool1/3x3_s2')(layer1)\n",
    "\n",
    "\n",
    "    for stack in range(4):\n",
    "        if stack == 0 :\n",
    "\n",
    "            if version == 'v2':\n",
    "                x = Conv2D(num_filters * 2, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_initializer='he_uniform', kernel_regularizer=l2(weight_decay))(cum_block)\n",
    "                x = BatchNormalization()(x) # 모든 conv.뒤에는 bn을 적용.\n",
    "                cum_block = block(input_shape=cum_block, num_filters=num_filters, reduction_ratio=reduction_ratio, identi=x, half=False, C=C)\n",
    "\n",
    "            # resnext에서는 아래 if문 수정\n",
    "            if version != 'v2':\n",
    "                  num_filters = num_filters * 2\n",
    "                  new_num_blocks = num_blocks[stack]\n",
    "            else :\n",
    "                  new_num_blocks = num_blocks[stack]-1\n",
    "\n",
    "            for res_block in range(new_num_blocks):\n",
    "                cum_block = block(input_shape=cum_block, num_filters=num_filters , reduction_ratio=reduction_ratio, identi=cum_block, half=False, C=C)\n",
    "\n",
    "            # resnext에서는 아래 if문 추가\n",
    "            if version != 'v2':\n",
    "                    num_filters = num_filters // 2\n",
    "    \n",
    "        else:\n",
    "\n",
    "            # 이 아래는 conv3_x ~ conv5_x 까지.\n",
    "            num_filters *= 2\n",
    " \n",
    "            if version == 'v2':\n",
    "                x = Conv2D(num_filters * 2, kernel_size=(1, 1), strides=(2, 2), padding='valid', kernel_initializer='he_uniform', kernel_regularizer=l2(weight_decay))(cum_block)  # block_v2라면, input에 해당하는 x를 덧셈이 가능하게 맞춰준다.\n",
    "                x = BatchNormalization()(x) # 모든 conv.뒤에는 bn을 적용.\n",
    "            else:\n",
    "                x = Conv2D(num_filters, (1, 1), strides=(2, 2), padding='valid', kernel_initializer='he_uniform', kernel_regularizer=l2(weight_decay))(cum_block)  # identi_input 에 해당 // rxc가 각각 반토막남.\n",
    "                x = BatchNormalization()(x) # 모든 conv.뒤에는 bn을 적용.\n",
    "                #즉, 위 x는 결국 이어질 conv 층의 input임.\n",
    "\n",
    "            # resnext에서는 아래 if문 추가\n",
    "            if version != 'v2':\n",
    "                    num_filters = num_filters * 2\n",
    "\n",
    "            for res_block in range(num_blocks[stack]):\n",
    "                if res_block == 0:\n",
    "                    cum_block = block(input_shape=cum_block , num_filters = num_filters , reduction_ratio=reduction_ratio, identi=x, half=True, C=C) # block 갱신\n",
    "                else:\n",
    "                    cum_block = block(input_shape=cum_block , num_filters = num_filters , reduction_ratio=reduction_ratio, identi=cum_block , half=False, C=C) # block 갱신\n",
    "\n",
    "            # resnext에서는 아래 if문 추가\n",
    "            if version != 'v2':\n",
    "                    num_filters = num_filters // 2\n",
    "\n",
    "\n",
    "    # 마지막 1층\n",
    "    y = GlobalAveragePooling2D()(cum_block)\n",
    "    outputs = Dense(classes, activation='softmax')(y)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name = name)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Y24Va7X1qmf"
   },
   "source": [
    "## 3. SE-ResNext18\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wiva9k1w1qmg"
   },
   "source": [
    "### 1) SE-ResNext18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WeiAeNmA1qmg"
   },
   "outputs": [],
   "source": [
    "# SE-ResNext18\n",
    "model = SE_ResNext(input_shape=input_sizes, classes=classes, num_filters=64, C=32, reduction_ratio=16, num_blocks=[2,2,2,2],version='v1',name='SE_ResNext18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pwRlyWzi1qmm"
   },
   "outputs": [],
   "source": [
    "# 폴더 생성\n",
    "\n",
    "os.makedirs(os.path.join(dir,'model_output',number,model.name), exist_ok=True)\n",
    "os.makedirs(os.path.join(dir,'train_valid_output',number), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8VpmuikFpZ0D"
   },
   "outputs": [],
   "source": [
    "# 참고 : https://github.com/OverLordGoldDragon/keras-adamw\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/Colab Notebooks/Paper')\n",
    "import utils\n",
    "import optimizers_v2\n",
    "from utils import get_weight_decays, fill_dict_in_order\n",
    "from utils import reset_seeds, K_eval\n",
    "from optimizers_v2 import AdamW, NadamW, SGDW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 193892,
     "status": "ok",
     "timestamp": 1599313813551,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "16331884730664529787"
     },
     "user_tz": -540
    },
    "id": "8wgOtwPW1qmp",
    "outputId": "e78980ce-e2aa-4448-b27a-a663fb2158b9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cosine annealing learning rates\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model=model, use_cosine_annealing=True, total_iterations = len(x_train) // batch_sizes , eta_min = 1e-2)\n",
    "#optimizer = Adam()\n",
    "filepath =  os.path.join(dir,'model_output',number,model.name,'{epoch:03d}.h5')\n",
    "\n",
    "callbacks_list = [ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_weights_only=False, save_best_only=True, mode='min'),\n",
    "                  ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_weights_only=False, save_best_only=True, mode='max'),\n",
    "                  #ReduceLROnPlateau(monitor='val_loss',patience=3,factor=0.1,min_lr=1e-5),\n",
    "                  LearningRateScheduler(lr_schedule,verbose=1)\n",
    "                  ]\n",
    "                  \n",
    "model.compile(optimizer, loss = 'categorical_crossentropy', metrics=['accuracy',macro_f1score,weighted_f1score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## flow_from_directory\n",
    "history = model.fit(train_generator, steps_per_epoch=int(len(x_train)/batch_sizes),  validation_data = valid_generator, epochs=epochs , verbose=1 , callbacks = callbacks_list , validation_steps=int(len(x_valid)/batch_sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FAr6jLIR1qmu"
   },
   "source": [
    "### 2) SE-ResNext18 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50365411,
     "status": "ok",
     "timestamp": 1599363985089,
     "user": {
      "displayName": "이동규",
      "photoUrl": "",
      "userId": "16331884730664529787"
     },
     "user_tz": -540
    },
    "id": "ZvKx1in81qmu",
    "outputId": "2bdceaaa-f5ed-4638-ff6f-19fc8d703b1c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 957s 20s/step - loss: 3.7568 - accuracy: 0.4912 - macro_f1score: 0.0944 - weighted_f1score: 0.0018\n",
      "[Test Loss: 3.7568 /  Test Accuracy: 0.4912 / Test Macro f1: 0.0944 / Test Weighted f1: 0.0018]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. epoch=maximum\n",
    "loss , acc, mf1, wf1 = model.evaluate(test_generator,steps=int(len(x_test)/batch_sizes))\n",
    "print('[Test Loss: %.4f /  Test Accuracy: %.4f / Test Macro f1: %.4f / Test Weighted f1: %.4f]\\n' % (loss,acc,mf1,wf1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fbdRXP391qmx"
   },
   "outputs": [],
   "source": [
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "acc=history.history['accuracy']\n",
    "val_acc=history.history['val_accuracy']\n",
    "f1=history.history['macro_f1score']\n",
    "val_f1=history.history['val_macro_f1score']\n",
    "epochs=range(1,len(acc)+1)\n",
    "\n",
    "data = np.array([epochs,loss,val_loss,acc,val_acc,f1,val_f1]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gRtTWKDk1qm0"
   },
   "outputs": [],
   "source": [
    "# data save\n",
    "# epochs, loss, val_loss, acc, val_acc, f1, val_f1\n",
    "\n",
    "np.savetxt(os.path.join(dir,'train_valid_output',number,model.name+'.txt'),data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3lB5x2Y_1qm3"
   },
   "outputs": [],
   "source": [
    "# data import\n",
    "data = np.loadtxt(os.path.join(dir,'train_valid_output',number,'SE_ResNext18.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQVReEQO1qm5"
   },
   "outputs": [],
   "source": [
    "epochs=data[:,0]\n",
    "loss=data[:,1]\n",
    "val_loss=data[:,2]\n",
    "acc=data[:,3]\n",
    "val_acc=data[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jkbe4ajm1qnG"
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs[1:],acc[1:],'b',label='Training Acc')\n",
    "plt.plot(epochs[1:],val_acc[1:],'r',label='Validation Acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs[10:],loss[10:],'b',label='Training Loss')\n",
    "plt.plot(epochs[10:],val_loss[10:],'r',label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XLDgyUtp1qnJ"
   },
   "outputs": [],
   "source": [
    "# 이 아래는 내가 인위적으로 살펴보고 싶은 epoch에 대해서 결과값 출력 / resnext18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model(os.path.join(dir,'model_output',number,'SE_ResNext18','026.h5'),custom_objects={\"macro_f1score\": macro_f1score,\"weighted_f1score\":weighted_f1score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I0IvhWkE1qnN"
   },
   "outputs": [],
   "source": [
    "# 2. epoch=?\n",
    "loss , acc, mf1, wf1 = model.evaluate(test_generator,steps=int(len(x_test)/batch_sizes))\n",
    "print('[Test Loss: %.4f /  Test Accuracy: %.4f / Test Macro f1: %.4f / Test Weighted f1: %.4f]\\n' % (loss,acc,mf1,wf1))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "13.SE-ResNext18.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
